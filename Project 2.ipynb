{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484f0846",
   "metadata": {},
   "source": [
    "# Project 2 - Big Data & NLP Analytics Setup\n",
    "\n",
    "This notebook sets up a comprehensive environment for:\n",
    "- **Big Data Processing**: PySpark, Kafka\n",
    "- **Natural Language Processing**: NLTK, spaCy, Transformers, TextBlob\n",
    "- **Machine Learning**: PyTorch, Scikit-learn\n",
    "- **Data Analysis**: Pandas, NumPy\n",
    "- **Visualization**: Matplotlib, Seaborn, Plotly, WordCloud\n",
    "- **Web Scraping**: BeautifulSoup, Tweepy, Requests\n",
    "- **Search & Storage**: Elasticsearch\n",
    "- **Web Applications**: Streamlit\n",
    "\n",
    "## Installation Process\n",
    "Run the cells below to install all required libraries with specific versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9261551a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.5.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pyspark==3.5.0) (0.10.9.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python==2.0.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (2.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confluent-kafka==2.3.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (2.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark==2.0.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (2.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.8.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from nltk==3.8.1) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from nltk==3.8.1) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from nltk==3.8.1) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from nltk==3.8.1) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk==3.8.1) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy==3.7.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (78.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from spacy==3.7.2) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.2) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2025.6.15)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy==3.7.2) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy==3.7.2) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==3.7.2) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy==3.7.2) (8.2.1)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy==3.7.2) (0.16.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.2) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from jinja2->spacy==3.7.2) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.35.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from transformers==4.35.2) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from transformers==4.35.2) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from transformers==4.35.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from transformers==4.35.2) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from transformers==4.35.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from transformers==4.35.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from transformers==4.35.2) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from transformers==4.35.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from transformers==4.35.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from transformers==4.35.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers==4.35.2) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->transformers==4.35.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->transformers==4.35.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->transformers==4.35.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->transformers==4.35.2) (2025.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement torch==2.1.1 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1)\n",
      "ERROR: No matching distribution found for torch==2.1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.3.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from scikit-learn==1.3.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from scikit-learn==1.3.2) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from scikit-learn==1.3.2) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from scikit-learn==1.3.2) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob==0.17.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (0.17.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from textblob==0.17.1) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from nltk>=3.1->textblob==0.17.1) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from nltk>=3.1->textblob==0.17.1) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from nltk>=3.1->textblob==0.17.1) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from nltk>=3.1->textblob==0.17.1) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk>=3.1->textblob==0.17.1) (0.4.6)\n",
      "Requirement already satisfied: vaderSentiment==3.3.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from vaderSentiment==3.3.2) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->vaderSentiment==3.3.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->vaderSentiment==3.3.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->vaderSentiment==3.3.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->vaderSentiment==3.3.2) (2025.6.15)\n",
      "Requirement already satisfied: vaderSentiment==3.3.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from vaderSentiment==3.3.2) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->vaderSentiment==3.3.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->vaderSentiment==3.3.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->vaderSentiment==3.3.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->vaderSentiment==3.3.2) (2025.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch==8.11.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (8.11.0)\n",
      "Requirement already satisfied: elastic-transport<9,>=8 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from elasticsearch==8.11.0) (8.17.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from elastic-transport<9,>=8->elasticsearch==8.11.0) (2.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from elastic-transport<9,>=8->elasticsearch==8.11.0) (2025.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch-dsl==8.11.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (8.11.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from elasticsearch-dsl==8.11.0) (2.9.0.post0)\n",
      "Requirement already satisfied: elasticsearch<9.0.0,>=8.0.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from elasticsearch-dsl==8.11.0) (8.11.0)\n",
      "Requirement already satisfied: elastic-transport<9,>=8 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from elasticsearch<9.0.0,>=8.0.0->elasticsearch-dsl==8.11.0) (8.17.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from elastic-transport<9,>=8->elasticsearch<9.0.0,>=8.0.0->elasticsearch-dsl==8.11.0) (2.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from elastic-transport<9,>=8->elasticsearch<9.0.0,>=8.0.0->elasticsearch-dsl==8.11.0) (2025.6.15)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil->elasticsearch-dsl==8.11.0) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.1.3 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (2.1.3)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pandas==2.1.3) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from pandas==2.1.3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pandas==2.1.3) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pandas==2.1.3) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas==2.1.3) (1.16.0)\n",
      "Collecting numpy==1.25.2\n",
      "  Using cached numpy-1.25.2.tar.gz (10.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n",
      "Collecting numpy==1.25.2\n",
      "  Using cached numpy-1.25.2.tar.gz (10.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [33 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 389, in <module>\n",
      "          main()\n",
      "        File \"C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 373, in main\n",
      "          json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 137, in get_requires_for_build_wheel\n",
      "          backend = _build_backend()\n",
      "                    ^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 70, in _build_backend\n",
      "          obj = import_module(mod_path)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "          return _bootstrap._gcd_import(name[level:], package, level)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "        File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "        File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "        File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "        File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "        File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "        File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "        File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "        File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
      "        File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "        File \"C:\\Users\\Honor\\AppData\\Local\\Temp\\pip-build-env-8ji88158\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 16, in <module>\n",
      "          import setuptools.version\n",
      "        File \"C:\\Users\\Honor\\AppData\\Local\\Temp\\pip-build-env-8ji88158\\overlay\\Lib\\site-packages\\setuptools\\version.py\", line 1, in <module>\n",
      "          import pkg_resources\n",
      "        File \"C:\\Users\\Honor\\AppData\\Local\\Temp\\pip-build-env-8ji88158\\overlay\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2172, in <module>\n",
      "          register_finder(pkgutil.ImpImporter, find_on_path)\n",
      "                          ^^^^^^^^^^^^^^^^^^^\n",
      "      AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib==3.8.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib==3.8.2) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib==3.8.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib==3.8.2) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib==3.8.2) (1.4.8)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib==3.8.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib==3.8.2) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib==3.8.2) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib==3.8.2) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib==3.8.2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib==3.8.2) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn==0.13.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from seaborn==0.13.0) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from seaborn==0.13.0) (2.1.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from seaborn==0.13.0) (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn==0.13.0) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn==0.13.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn==0.13.0) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn==0.13.0) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn==0.13.0) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn==0.13.0) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn==0.13.0) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn==0.13.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pandas>=1.2->seaborn==0.13.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pandas>=1.2->seaborn==0.13.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn==0.13.0) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly==5.17.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (5.17.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from plotly==5.17.0) (9.1.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from plotly==5.17.0) (24.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests==2.31.0\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests==2.31.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests==2.31.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests==2.31.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests==2.31.0) (2025.6.15)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Installing collected packages: requests\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.4\n",
      "    Uninstalling requests-2.32.4:\n",
      "      Successfully uninstalled requests-2.32.4\n",
      "Successfully installed requests-2.31.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "openai 0.28.0 requires aiohttp, which is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4==4.12.2\n",
      "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4==4.12.2)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "\n",
      "   ---------------------------------------- 0/2 [soupsieve]\n",
      "   ---------------------------------------- 0/2 [soupsieve]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   ---------------------------------------- 2/2 [beautifulsoup4]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.12.2 soupsieve-2.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy==4.14.0\n",
      "  Downloading tweepy-4.14.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting oauthlib<4,>=3.2.0 (from tweepy==4.14.0)\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from tweepy==4.14.0) (2.31.0)\n",
      "Collecting requests-oauthlib<2,>=1.2.0 (from tweepy==4.14.0)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3,>=2.27.0->tweepy==4.14.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3,>=2.27.0->tweepy==4.14.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3,>=2.27.0->tweepy==4.14.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3,>=2.27.0->tweepy==4.14.0) (2025.6.15)\n",
      "Downloading tweepy-4.14.0-py3-none-any.whl (98 kB)\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ------------- -------------------------- 1/3 [requests-oauthlib]\n",
      "   ------------- -------------------------- 1/3 [requests-oauthlib]\n",
      "   ------------- -------------------------- 1/3 [requests-oauthlib]\n",
      "   -------------------------- ------------- 2/3 [tweepy]\n",
      "   -------------------------- ------------- 2/3 [tweepy]\n",
      "   -------------------------- ------------- 2/3 [tweepy]\n",
      "   -------------------------- ------------- 2/3 [tweepy]\n",
      "   -------------------------- ------------- 2/3 [tweepy]\n",
      "   -------------------------- ------------- 2/3 [tweepy]\n",
      "   ---------------------------------------- 3/3 [tweepy]\n",
      "\n",
      "Successfully installed oauthlib-3.3.1 requests-oauthlib-1.3.1 tweepy-4.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv==1.0.0\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud==1.9.2\n",
      "  Downloading wordcloud-1.9.2.tar.gz (222 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [16 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 35, in <module>\n",
      "        File \"C:\\Users\\Honor\\AppData\\Local\\Temp\\pip-install-8hs0zz6k\\wordcloud_0c1e7efd406048149227bfd28aeaff34\\setup.py\", line 13, in <module>\n",
      "          version=versioneer.get_version(),\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Honor\\AppData\\Local\\Temp\\pip-install-8hs0zz6k\\wordcloud_0c1e7efd406048149227bfd28aeaff34\\versioneer.py\", line 1480, in get_version\n",
      "          return get_versions()[\"version\"]\n",
      "                 ^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Honor\\AppData\\Local\\Temp\\pip-install-8hs0zz6k\\wordcloud_0c1e7efd406048149227bfd28aeaff34\\versioneer.py\", line 1412, in get_versions\n",
      "          cfg = get_config_from_root(root)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Honor\\AppData\\Local\\Temp\\pip-install-8hs0zz6k\\wordcloud_0c1e7efd406048149227bfd28aeaff34\\versioneer.py\", line 342, in get_config_from_root\n",
      "          parser = configparser.SafeConfigParser()\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "      AttributeError: module 'configparser' has no attribute 'SafeConfigParser'. Did you mean: 'RawConfigParser'?\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit==1.28.2\n",
      "  Downloading streamlit-1.28.2-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit==1.28.2)\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit==1.28.2)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit==1.28.2)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from streamlit==1.28.2) (8.2.1)\n",
      "Collecting importlib-metadata<7,>=1.4 (from streamlit==1.28.2)\n",
      "  Downloading importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from streamlit==1.28.2) (1.26.4)\n",
      "Collecting packaging<24,>=16.8 (from streamlit==1.28.2)\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from streamlit==1.28.2) (2.1.3)\n",
      "Collecting pillow<11,>=7.1.0 (from streamlit==1.28.2)\n",
      "  Downloading pillow-10.4.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting protobuf<5,>=3.20 (from streamlit==1.28.2)\n",
      "  Downloading protobuf-4.25.8-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting pyarrow>=6.0 (from streamlit==1.28.2)\n",
      "  Downloading pyarrow-20.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from streamlit==1.28.2) (2.9.0.post0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from streamlit==1.28.2) (2.31.0)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit==1.28.2)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tenacity<9,>=8.1.0 (from streamlit==1.28.2)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit==1.28.2)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from streamlit==1.28.2) (4.14.0)\n",
      "Collecting tzlocal<6,>=1.1 (from streamlit==1.28.2)\n",
      "  Downloading tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting validators<1,>=0.2 (from streamlit==1.28.2)\n",
      "  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit==1.28.2)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit==1.28.2)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from streamlit==1.28.2) (6.4)\n",
      "Collecting watchdog>=2.1.5 (from streamlit==1.28.2)\n",
      "  Downloading watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from altair<6,>=4.0->streamlit==1.28.2) (3.1.6)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit==1.28.2)\n",
      "  Downloading jsonschema-4.24.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from altair<6,>=4.0->streamlit==1.28.2) (1.43.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from click<9,>=7.0->streamlit==1.28.2) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.28.2)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.28.2)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<7,>=1.4->streamlit==1.28.2)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit==1.28.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit==1.28.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil<3,>=2.7.3->streamlit==1.28.2) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3,>=2.27->streamlit==1.28.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3,>=2.27->streamlit==1.28.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3,>=2.27->streamlit==1.28.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3,>=2.27->streamlit==1.28.2) (2025.6.15)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from rich<14,>=10.14.0->streamlit==1.28.2) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from rich<14,>=10.14.0->streamlit==1.28.2) (2.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit==1.28.2) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.28.2) (25.3.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.28.2)\n",
      "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.28.2)\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.28.2)\n",
      "  Downloading rpds_py-0.25.1-cp312-cp312-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit==1.28.2) (0.1.2)\n",
      "Downloading streamlit-1.28.2-py2.py3-none-any.whl (8.4 MB)\n",
      "   ---------------------------------------- 0.0/8.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/8.4 MB 10.1 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.1/8.4 MB 4.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.4/8.4 MB 3.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.1/8.4 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.2/8.4 MB 4.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.7/8.4 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.2/8.4 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.3/8.4 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.3/8.4 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.3/8.4 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.3/8.4 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.3/8.4 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.3/8.4 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.6/8.4 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.4/8.4 MB 2.5 MB/s eta 0:00:00\n",
      "Downloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "   ---------------------------------------- 0.0/731.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 731.2/731.2 kB 6.0 MB/s eta 0:00:00\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Downloading pillow-10.4.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 2.4/2.6 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 8.2 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.8-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "   ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 1.6/6.9 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.1/6.9 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.9/6.9 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.4/6.9 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.2/6.9 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.2/6.9 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.3/6.9 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.9/6.9 MB 4.1 MB/s eta 0:00:00\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Downloading validators-0.35.0-py3-none-any.whl (44 kB)\n",
      "Downloading jsonschema-4.24.0-py3-none-any.whl (88 kB)\n",
      "Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Downloading pyarrow-20.0.0-cp312-cp312-win_amd64.whl (25.7 MB)\n",
      "   ---------------------------------------- 0.0/25.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/25.7 MB 6.3 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.6/25.7 MB 6.3 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.6/25.7 MB 6.3 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.6/25.7 MB 6.3 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.6/25.7 MB 6.3 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 3.4/25.7 MB 2.6 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 4.2/25.7 MB 2.8 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 5.2/25.7 MB 3.0 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 5.2/25.7 MB 3.0 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 5.2/25.7 MB 3.0 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 5.2/25.7 MB 3.0 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 5.5/25.7 MB 2.1 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.3/25.7 MB 2.2 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.1/25.7 MB 2.3 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.4/25.7 MB 2.5 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 9.4/25.7 MB 2.7 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 11.0/25.7 MB 2.9 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 12.3/25.7 MB 3.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 13.6/25.7 MB 3.3 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 14.9/25.7 MB 3.4 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 15.7/25.7 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 16.5/25.7 MB 3.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 17.0/25.7 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 17.6/25.7 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 18.1/25.7 MB 3.3 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 18.9/25.7 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.9/25.7 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 21.0/25.7 MB 3.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 22.3/25.7 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.3/25.7 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.4/25.7 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.2/25.7 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.7/25.7 MB 3.6 MB/s eta 0:00:00\n",
      "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.25.1-cp312-cp312-win_amd64.whl (235 kB)\n",
      "Downloading watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: zipp, watchdog, validators, tzlocal, toml, tenacity, smmap, rpds-py, pyarrow, protobuf, pillow, packaging, cachetools, blinker, rich, referencing, pydeck, importlib-metadata, gitdb, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
      "\n",
      "   - --------------------------------------  1/24 [watchdog]\n",
      "   - --------------------------------------  1/24 [watchdog]\n",
      "   - --------------------------------------  1/24 [watchdog]\n",
      "   - --------------------------------------  1/24 [watchdog]\n",
      "   - --------------------------------------  1/24 [watchdog]\n",
      "   --- ------------------------------------  2/24 [validators]\n",
      "   --- ------------------------------------  2/24 [validators]\n",
      "   --- ------------------------------------  2/24 [validators]\n",
      "   --- ------------------------------------  2/24 [validators]\n",
      "   --- ------------------------------------  2/24 [validators]\n",
      "   --- ------------------------------------  2/24 [validators]\n",
      "   ----- ----------------------------------  3/24 [tzlocal]\n",
      "  Attempting uninstall: tenacity\n",
      "   ----- ----------------------------------  3/24 [tzlocal]\n",
      "    Found existing installation: tenacity 9.1.2\n",
      "   ----- ----------------------------------  3/24 [tzlocal]\n",
      "    Uninstalling tenacity-9.1.2:\n",
      "   ----- ----------------------------------  3/24 [tzlocal]\n",
      "      Successfully uninstalled tenacity-9.1.2\n",
      "   ----- ----------------------------------  3/24 [tzlocal]\n",
      "   -------- -------------------------------  5/24 [tenacity]\n",
      "   -------- -------------------------------  5/24 [tenacity]\n",
      "   ---------- -----------------------------  6/24 [smmap]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "  Attempting uninstall: pillow\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "    Found existing installation: pillow 11.2.1\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "    Uninstalling pillow-11.2.1:\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "      Successfully uninstalled pillow-11.2.1\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "  Attempting uninstall: packaging\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "    Found existing installation: packaging 24.0\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "    Uninstalling packaging-24.0:\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "      Successfully uninstalled packaging-24.0\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ------------------ --------------------- 11/24 [packaging]\n",
      "   ------------------ --------------------- 11/24 [packaging]\n",
      "   -------------------- ------------------- 12/24 [cachetools]\n",
      "  Attempting uninstall: rich\n",
      "   -------------------- ------------------- 12/24 [cachetools]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "    Found existing installation: rich 14.0.0\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "    Uninstalling rich-14.0.0:\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "      Successfully uninstalled rich-14.0.0\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ------------------------- -------------- 15/24 [referencing]\n",
      "   ------------------------- -------------- 15/24 [referencing]\n",
      "   -------------------------- ------------- 16/24 [pydeck]\n",
      "   -------------------------- ------------- 16/24 [pydeck]\n",
      "   -------------------------- ------------- 16/24 [pydeck]\n",
      "   -------------------------- ------------- 16/24 [pydeck]\n",
      "   -------------------------- ------------- 16/24 [pydeck]\n",
      "   ---------------------------- ----------- 17/24 [importlib-metadata]\n",
      "   ------------------------------ --------- 18/24 [gitdb]\n",
      "   ------------------------------ --------- 18/24 [gitdb]\n",
      "   ------------------------------ --------- 18/24 [gitdb]\n",
      "   ------------------------------ --------- 18/24 [gitdb]\n",
      "   ------------------------------- -------- 19/24 [jsonschema-specifications]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   ----------------------------------- ---- 21/24 [jsonschema]\n",
      "   ----------------------------------- ---- 21/24 [jsonschema]\n",
      "   ----------------------------------- ---- 21/24 [jsonschema]\n",
      "   ----------------------------------- ---- 21/24 [jsonschema]\n",
      "   ----------------------------------- ---- 21/24 [jsonschema]\n",
      "   ----------------------------------- ---- 21/24 [jsonschema]\n",
      "   ----------------------------------- ---- 21/24 [jsonschema]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   ---------------------------------------- 24/24 [streamlit]\n",
      "\n",
      "Successfully installed altair-5.5.0 blinker-1.9.0 cachetools-5.5.2 gitdb-4.0.12 gitpython-3.1.44 importlib-metadata-6.11.0 jsonschema-4.24.0 jsonschema-specifications-2025.4.1 packaging-23.2 pillow-10.4.0 protobuf-4.25.8 pyarrow-20.0.0 pydeck-0.9.1 referencing-0.36.2 rich-13.9.4 rpds-py-0.25.1 smmap-5.0.2 streamlit-1.28.2 tenacity-8.5.0 toml-0.10.2 tzlocal-5.3.1 validators-0.35.0 watchdog-6.0.0 zipp-3.23.0\n",
      "✅ Installation completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "modin 0.32.0 requires pandas<2.3,>=2.2, but you have pandas 2.1.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Simple Installation - Run this cell to install all libraries\n",
    "!pip install pyspark==3.5.0\n",
    "!pip install kafka-python==2.0.2\n",
    "!pip install confluent-kafka==2.3.0\n",
    "!pip install findspark==2.0.1\n",
    "!pip install nltk==3.8.1\n",
    "!pip install spacy==3.7.2\n",
    "!pip install transformers==4.35.2\n",
    "!pip install torch==2.1.1\n",
    "!pip install scikit-learn==1.3.2\n",
    "!pip install textblob==0.17.1\n",
    "!pip install vaderSentiment==3.3.2\n",
    "!pip install elasticsearch==8.11.0\n",
    "!pip install elasticsearch-dsl==8.11.0\n",
    "!pip install pandas==2.1.3\n",
    "!pip install numpy==1.25.2\n",
    "!pip install matplotlib==3.8.2\n",
    "!pip install seaborn==0.13.0\n",
    "\n",
    "!pip install plotly==5.17.0\n",
    "!pip install requests==2.31.0\n",
    "!pip install beautifulsoup4==4.12.2\n",
    "!pip install tweepy==4.14.0\n",
    "!pip install python-dotenv==1.0.0\n",
    "!pip install wordcloud==1.9.2\n",
    "!pip install streamlit==1.28.2\n",
    "\n",
    "print(\"✅ Installation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb624fc8",
   "metadata": {},
   "source": [
    "# 🇲🇾 Malaysian Reddit Sentiment Analysis Pipeline\n",
    "\n",
    "## Project Overview\n",
    "This project (worth 15% of course assessment) builds a real-time sentiment analysis pipeline monitoring public sentiment from r/malaysia subreddit. The system processes large-scale text data using Apache technologies, classifies sentiment, and visualizes insights through interactive dashboards.\n",
    "\n",
    "## 📋 Project Specifications\n",
    "- **Deadline**: Friday, June 27, 2025\n",
    "- **Team Size**: Maximum 4 students per group\n",
    "- **Data Source**: [r/malaysia](https://www.reddit.com/r/malaysia/)\n",
    "\n",
    "## 🎯 Core Objectives\n",
    "1. Monitor public sentiment in real-time from r/malaysia\n",
    "2. Process Reddit data using Apache Spark and Kafka\n",
    "3. Classify sentiment as positive, negative, or neutral\n",
    "4. Visualize insights through interactive dashboards\n",
    "\n",
    "## 📚 Components in this Notebook\n",
    "1. **Data Collection**: Streaming data from Reddit API\n",
    "2. **Data Processing Pipeline**: Kafka + Spark integration\n",
    "3. **Sentiment Analysis**: Multiple model implementations\n",
    "4. **Storage**: Elasticsearch integration \n",
    "5. **Visualization**: Real-time dashboards\n",
    "\n",
    "Let's start building our pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6928e1e",
   "metadata": {},
   "source": [
    "## 1. Reddit API Setup\n",
    "\n",
    "To collect data from r/malaysia, we'll need to set up Reddit API credentials. This section:\n",
    "1. Sets up authentication with PRAW (Python Reddit API Wrapper)\n",
    "2. Configures environment variables for secure credential storage\n",
    "3. Tests the connection to ensure we can access r/malaysia\n",
    "\n",
    "First, you'll need to create a Reddit API application:\n",
    "1. Go to https://www.reddit.com/prefs/apps\n",
    "2. Click \"create another app...\" at the bottom\n",
    "3. Fill in the details (name, description)\n",
    "4. Select \"script\" as the app type\n",
    "5. For \"redirect uri\" use http://localhost:8080\n",
    "6. Click \"create app\"\n",
    "\n",
    "You'll receive a client_id (under \"personal use script\") and client_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a37aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a .env file to store credentials\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up the .env file path\n",
    "env_path = Path('.') / '.env'\n",
    "\n",
    "# Check if .env file exists, create if it doesn't\n",
    "if not os.path.exists(env_path):\n",
    "    # Use your own credentials here\n",
    "    with open(env_path, 'w') as f:\n",
    "        f.write(\"\"\"\n",
    "# Reddit API Credentials\n",
    "REDDIT_CLIENT_ID=your_client_id_here\n",
    "REDDIT_CLIENT_SECRET=your_client_secret_here\n",
    "REDDIT_USER_AGENT=MyApp/1.0.0 (by /u/your_username)\n",
    "REDDIT_USERNAME=your_username\n",
    "REDDIT_PASSWORD=your_password\n",
    "\n",
    "# Elasticsearch Configuration\n",
    "ES_HOST=localhost\n",
    "ES_PORT=9200\n",
    "        \"\"\")\n",
    "    print(f\"✅ Created .env file at {env_path.absolute()}\")\n",
    "    print(\"⚠️ Please edit the .env file and update your Reddit API credentials\")\n",
    "else:\n",
    "    print(f\"✅ Found existing .env file at {env_path.absolute()}\")\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Display available environment variables (masking sensitive info)\n",
    "print(\"\\n--- Available Environment Variables ---\")\n",
    "for key in os.environ:\n",
    "    if key.startswith(('REDDIT_', 'ES_')):\n",
    "        if 'SECRET' in key or 'PASSWORD' in key:\n",
    "            value = \"********\" \n",
    "        else:\n",
    "            value = os.environ[key]\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53184ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connection to Reddit API\n",
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    # Initialize Reddit API connection\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=os.environ.get('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.environ.get('REDDIT_CLIENT_SECRET'),\n",
    "        user_agent=os.environ.get('REDDIT_USER_AGENT'),\n",
    "        username=os.environ.get('REDDIT_USERNAME'),\n",
    "        password=os.environ.get('REDDIT_PASSWORD')\n",
    "    )\n",
    "    \n",
    "    # Test connection by fetching hot posts from r/malaysia\n",
    "    subreddit = reddit.subreddit('malaysia')\n",
    "    hot_posts = subreddit.hot(limit=5)\n",
    "    \n",
    "    # Convert to dataframe for display\n",
    "    posts_data = []\n",
    "    \n",
    "    for post in hot_posts:\n",
    "        posts_data.append({\n",
    "            'title': post.title,\n",
    "            'score': post.score,\n",
    "            'id': post.id,\n",
    "            'created_utc': pd.to_datetime(post.created_utc, unit='s'),\n",
    "            'num_comments': post.num_comments,\n",
    "            'permalink': post.permalink\n",
    "        })\n",
    "    \n",
    "    posts_df = pd.DataFrame(posts_data)\n",
    "    \n",
    "    print(\"✅ Successfully connected to Reddit API\")\n",
    "    print(f\"✅ Fetched {len(posts_data)} posts from r/malaysia\\n\")\n",
    "    print(\"Sample posts from r/malaysia:\")\n",
    "    display(posts_df)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"❌ Error connecting to Reddit API:\")\n",
    "    print(f\"Error message: {e}\")\n",
    "    print(\"\\n⚠️ Please check your Reddit API credentials in the .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f82fd",
   "metadata": {},
   "source": [
    "## 2. Apache Kafka Setup\n",
    "\n",
    "For real-time data streaming, we'll use Apache Kafka. This section:\n",
    "1. Sets up a local Kafka environment\n",
    "2. Creates a topic for r/malaysia data\n",
    "3. Implements a producer to stream data from Reddit\n",
    "4. Implements a consumer to process the streaming data\n",
    "\n",
    "For a full implementation, you would need a running Kafka instance. \n",
    "For development purposes, we'll use a simulated Kafka environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6fc4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka Producer for Reddit Data\n",
    "import json\n",
    "import time\n",
    "from kafka import KafkaProducer\n",
    "from kafka.errors import KafkaError\n",
    "\n",
    "# Define Kafka configuration\n",
    "kafka_config = {\n",
    "    'bootstrap_servers': ['localhost:9092'],  # Change if your Kafka server is elsewhere\n",
    "    'topic_name': 'reddit_malaysia'\n",
    "}\n",
    "\n",
    "# Function to create a Kafka producer\n",
    "def create_kafka_producer():\n",
    "    try:\n",
    "        # Create producer instance\n",
    "        producer = KafkaProducer(\n",
    "            bootstrap_servers=kafka_config['bootstrap_servers'],\n",
    "            value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "            acks='all'\n",
    "        )\n",
    "        print(f\"✅ Connected to Kafka at {kafka_config['bootstrap_servers']}\")\n",
    "        return producer\n",
    "    except KafkaError as e:\n",
    "        print(f\"❌ Failed to connect to Kafka: {e}\")\n",
    "        print(\"ℹ️ If you don't have Kafka running, we'll use a simulated environment\")\n",
    "        return None\n",
    "\n",
    "# Function to stream Reddit posts to Kafka\n",
    "def stream_reddit_to_kafka(producer, subreddit_name='malaysia', limit=100):\n",
    "    if producer is None:\n",
    "        print(\"⚠️ Using simulated Kafka environment (no real Kafka available)\")\n",
    "        producer = SimulatedKafkaProducer()\n",
    "    \n",
    "    print(f\"🔄 Starting to stream data from r/{subreddit_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize Reddit API\n",
    "        reddit = praw.Reddit(\n",
    "            client_id=os.environ.get('REDDIT_CLIENT_ID'),\n",
    "            client_secret=os.environ.get('REDDIT_CLIENT_SECRET'),\n",
    "            user_agent=os.environ.get('REDDIT_USER_AGENT'),\n",
    "            username=os.environ.get('REDDIT_USERNAME'),\n",
    "            password=os.environ.get('REDDIT_PASSWORD')\n",
    "        )\n",
    "        \n",
    "        # Get subreddit\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        \n",
    "        # Stream the posts\n",
    "        for i, post in enumerate(subreddit.hot(limit=limit)):\n",
    "            # Extract post data\n",
    "            post_data = {\n",
    "                'id': post.id,\n",
    "                'title': post.title,\n",
    "                'author': str(post.author),\n",
    "                'selftext': post.selftext,\n",
    "                'score': post.score,\n",
    "                'created_utc': post.created_utc,\n",
    "                'num_comments': post.num_comments,\n",
    "                'url': post.url,\n",
    "                'timestamp': time.time()\n",
    "            }\n",
    "            \n",
    "            # Send to Kafka\n",
    "            future = producer.send(kafka_config['topic_name'], post_data)\n",
    "            \n",
    "            # Get metadata about the record\n",
    "            try:\n",
    "                record_metadata = future.get(timeout=10)\n",
    "                print(f\"✅ Post {i+1}/{limit} sent to {record_metadata.topic} partition {record_metadata.partition}\")\n",
    "            except KafkaError as e:\n",
    "                print(f\"❌ Error sending post: {e}\")\n",
    "            \n",
    "            # Small delay to simulate real-time streaming\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        print(f\"✅ Completed streaming {limit} posts from r/{subreddit_name}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in streaming: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # Close producer\n",
    "        if producer is not None and not isinstance(producer, SimulatedKafkaProducer):\n",
    "            producer.close()\n",
    "            print(\"✅ Kafka producer closed\")\n",
    "\n",
    "# Simulated Kafka Producer for development without real Kafka\n",
    "class SimulatedKafkaProducer:\n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "        print(\"✓ Initialized simulated Kafka producer\")\n",
    "        \n",
    "    def send(self, topic, value):\n",
    "        self.messages.append((topic, value))\n",
    "        print(f\"✓ Simulated send to {topic}: {value.get('title', '')}...\")\n",
    "        \n",
    "        # Return a simulated future\n",
    "        class SimulatedFuture:\n",
    "            def get(self, timeout=None):\n",
    "                class SimulatedMetadata:\n",
    "                    def __init__(self, topic, partition):\n",
    "                        self.topic = topic\n",
    "                        self.partition = partition\n",
    "                return SimulatedMetadata(topic, 0)\n",
    "        \n",
    "        return SimulatedFuture()\n",
    "    \n",
    "    def close(self):\n",
    "        print(f\"✓ Simulated producer closed. {len(self.messages)} messages sent.\")\n",
    "\n",
    "# Create a producer (or simulated producer if Kafka isn't running)\n",
    "producer = create_kafka_producer()\n",
    "\n",
    "# For test purposes, stream a limited number of posts\n",
    "stream_reddit_to_kafka(producer, 'malaysia', limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e7abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka Consumer with Spark Integration\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import findspark\n",
    "\n",
    "# Initialize findspark to locate PySpark\n",
    "findspark.init()\n",
    "\n",
    "# Create a Spark Session\n",
    "def create_spark_session(app_name=\"RedditSentimentAnalysis\"):\n",
    "    try:\n",
    "        spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(app_name) \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "            .config(\"spark.executor.memory\", \"1g\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .getOrCreate()\n",
    "            \n",
    "        print(f\"✅ Spark session created: {spark.version}\")\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to create Spark session: {e}\")\n",
    "        return None\n",
    "\n",
    "# Kafka Consumer for Reddit data\n",
    "def create_kafka_consumer():\n",
    "    try:\n",
    "        # Create consumer\n",
    "        consumer = KafkaConsumer(\n",
    "            kafka_config['topic_name'],\n",
    "            bootstrap_servers=kafka_config['bootstrap_servers'],\n",
    "            auto_offset_reset='earliest',\n",
    "            enable_auto_commit=True,\n",
    "            group_id='reddit-sentiment-group',\n",
    "            value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    "        )\n",
    "        print(f\"✅ Connected to Kafka consumer group: reddit-sentiment-group\")\n",
    "        return consumer\n",
    "    except KafkaError as e:\n",
    "        print(f\"❌ Failed to create Kafka consumer: {e}\")\n",
    "        print(\"ℹ️ Using simulated data instead\")\n",
    "        return None\n",
    "\n",
    "# If we can't create a real consumer, use the data from the producer\n",
    "def process_messages(consumer=None, simulated_data=None, max_messages=10):\n",
    "    # Initialize Spark\n",
    "    spark = create_spark_session()\n",
    "    \n",
    "    # Collection to store all messages\n",
    "    all_messages = []\n",
    "    \n",
    "    if consumer is not None:\n",
    "        print(\"🔄 Consuming messages from Kafka...\")\n",
    "        \n",
    "        # Poll for messages\n",
    "        message_count = 0\n",
    "        for message in consumer:\n",
    "            message_data = message.value\n",
    "            print(f\"✅ Message received: {message_data.get('title', '')}\")\n",
    "            all_messages.append(message_data)\n",
    "            \n",
    "            message_count += 1\n",
    "            if message_count >= max_messages:\n",
    "                break\n",
    "                \n",
    "        print(f\"✅ Consumed {message_count} messages from Kafka\")\n",
    "    \n",
    "    elif simulated_data is not None:\n",
    "        print(\"ℹ️ Using simulated data...\")\n",
    "        all_messages = [msg[1] for msg in simulated_data.messages]\n",
    "        print(f\"✅ Loaded {len(all_messages)} simulated messages\")\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ No data source available\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to DataFrame for processing\n",
    "    if all_messages:\n",
    "        # Convert to pandas DataFrame first\n",
    "        pdf = pd.DataFrame(all_messages)\n",
    "        \n",
    "        if spark is not None:\n",
    "            # Convert to Spark DataFrame\n",
    "            df = spark.createDataFrame(pdf)\n",
    "            print(\"✅ Created Spark DataFrame with schema:\")\n",
    "            df.printSchema()\n",
    "            \n",
    "            # Show sample data\n",
    "            print(\"\\n✅ Sample data:\")\n",
    "            df.show(5, truncate=False)\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            print(\"⚠️ Spark not available, using pandas DataFrame instead\")\n",
    "            print(\"✅ Created pandas DataFrame with columns:\")\n",
    "            print(pdf.columns.tolist())\n",
    "            \n",
    "            # Show sample data\n",
    "            print(\"\\n✅ Sample data:\")\n",
    "            display(pdf.head())\n",
    "            \n",
    "            return pdf\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Try to create a consumer\n",
    "consumer = create_kafka_consumer()\n",
    "\n",
    "# Process messages - either from Kafka or simulated data\n",
    "# If we have a simulated producer with data, use that\n",
    "if consumer is None and isinstance(producer, SimulatedKafkaProducer) and producer.messages:\n",
    "    df = process_messages(simulated_data=producer, max_messages=10)\n",
    "else:\n",
    "    df = process_messages(consumer=consumer, max_messages=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e725aac",
   "metadata": {},
   "source": [
    "## 3. Sentiment Analysis\n",
    "\n",
    "This section implements multiple sentiment analysis models to analyze Reddit posts:\n",
    "\n",
    "1. **VADER** (Valence Aware Dictionary and sEntiment Reasoner): A lexicon and rule-based model specifically for social media content\n",
    "2. **TextBlob**: Simple but effective sentiment analysis library\n",
    "3. **Hugging Face Transformers**: State-of-the-art deep learning models\n",
    "4. **Custom ML model**: Training our own model using scikit-learn\n",
    "\n",
    "We'll compare these different approaches to see which performs best for Malaysian content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792724ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 VADER Sentiment Analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define function to analyze sentiment using VADER\n",
    "def analyze_sentiment_vader(text):\n",
    "    if not text or pd.isna(text) or text == '':\n",
    "        return {\n",
    "            'compound': 0.0,\n",
    "            'pos': 0.0,\n",
    "            'neu': 0.0,\n",
    "            'neg': 0.0,\n",
    "            'sentiment': 'neutral'\n",
    "        }\n",
    "    \n",
    "    # Get sentiment scores\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    \n",
    "    # Add sentiment label\n",
    "    if sentiment['compound'] >= 0.05:\n",
    "        sentiment['sentiment'] = 'positive'\n",
    "    elif sentiment['compound'] <= -0.05:\n",
    "        sentiment['sentiment'] = 'negative'\n",
    "    else:\n",
    "        sentiment['sentiment'] = 'neutral'\n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "# Process the dataframe we received from Kafka\n",
    "def process_sentiment_vader(df):\n",
    "    # Create a copy for processing\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        # We're working with pandas DataFrame\n",
    "        result_df = df.copy()\n",
    "        \n",
    "        # Apply sentiment analysis to titles\n",
    "        print(\"Analyzing sentiment of titles...\")\n",
    "        title_sentiments = result_df['title'].apply(analyze_sentiment_vader)\n",
    "        \n",
    "        # Extract sentiment components\n",
    "        result_df['title_compound'] = title_sentiments.apply(lambda x: x['compound'])\n",
    "        result_df['title_sentiment'] = title_sentiments.apply(lambda x: x['sentiment'])\n",
    "        \n",
    "        # Apply sentiment analysis to post content if available\n",
    "        if 'selftext' in result_df.columns:\n",
    "            print(\"Analyzing sentiment of post content...\")\n",
    "            content_sentiments = result_df['selftext'].apply(analyze_sentiment_vader)\n",
    "            result_df['content_compound'] = content_sentiments.apply(lambda x: x['compound'])\n",
    "            result_df['content_sentiment'] = content_sentiments.apply(lambda x: x['sentiment'])\n",
    "        \n",
    "        # Calculate overall sentiment (average of title and content)\n",
    "        if 'content_compound' in result_df.columns:\n",
    "            result_df['overall_compound'] = (result_df['title_compound'] + result_df['content_compound']) / 2\n",
    "            \n",
    "            # Apply sentiment label\n",
    "            result_df['overall_sentiment'] = result_df['overall_compound'].apply(\n",
    "                lambda x: 'positive' if x >= 0.05 else ('negative' if x <= -0.05 else 'neutral')\n",
    "            )\n",
    "        else:\n",
    "            result_df['overall_compound'] = result_df['title_compound']\n",
    "            result_df['overall_sentiment'] = result_df['title_sentiment']\n",
    "        \n",
    "        print(f\"✅ VADER sentiment analysis completed for {len(result_df)} posts\")\n",
    "        return result_df\n",
    "    else:\n",
    "        # We're working with PySpark DataFrame\n",
    "        # This would be implemented differently for PySpark\n",
    "        # For simplicity we'll convert to pandas first\n",
    "        result_df = df.toPandas()\n",
    "        \n",
    "        # Process with the pandas method\n",
    "        result_df = process_sentiment_vader(result_df)\n",
    "        \n",
    "        # Convert back to Spark DataFrame\n",
    "        spark = create_spark_session()\n",
    "        if spark:\n",
    "            return spark.createDataFrame(result_df)\n",
    "        else:\n",
    "            return result_df\n",
    "\n",
    "# Apply sentiment analysis to our data\n",
    "if df is not None:\n",
    "    sentiment_df = process_sentiment_vader(df)\n",
    "    \n",
    "    # Display results\n",
    "    if isinstance(sentiment_df, pd.DataFrame):\n",
    "        print(\"\\n--- Sentiment Analysis Results ---\")\n",
    "        display(sentiment_df[['title', 'overall_sentiment', 'overall_compound']])\n",
    "        \n",
    "        # Create a chart to visualize sentiment distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sentiment_counts = sentiment_df['overall_sentiment'].value_counts()\n",
    "        sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette={'positive': 'green', 'neutral': 'blue', 'negative': 'red'})\n",
    "        plt.title('Overall Sentiment Distribution in r/malaysia Posts')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xlabel('Sentiment')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\n--- Sentiment Analysis Results ---\")\n",
    "        sentiment_df.select('title', 'overall_sentiment', 'overall_compound').show(10, truncate=False)\n",
    "        \n",
    "        # Get counts for visualization\n",
    "        sentiment_counts = sentiment_df.groupBy('overall_sentiment').count().toPandas()\n",
    "        \n",
    "        # Create a chart\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='overall_sentiment', y='count', data=sentiment_counts, palette={'positive': 'green', 'neutral': 'blue', 'negative': 'red'})\n",
    "        plt.title('Overall Sentiment Distribution in r/malaysia Posts')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xlabel('Sentiment')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"❌ No data available for sentiment analysis. Please run the previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0362089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 TextBlob Sentiment Analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Define function to analyze sentiment using TextBlob\n",
    "def analyze_sentiment_textblob(text):\n",
    "    if not text or pd.isna(text) or text == '':\n",
    "        return {\n",
    "            'polarity': 0.0,\n",
    "            'subjectivity': 0.0,\n",
    "            'sentiment': 'neutral'\n",
    "        }\n",
    "    \n",
    "    # Create TextBlob object\n",
    "    blob = TextBlob(text)\n",
    "    \n",
    "    # Get sentiment scores\n",
    "    sentiment = {\n",
    "        'polarity': blob.sentiment.polarity,\n",
    "        'subjectivity': blob.sentiment.subjectivity\n",
    "    }\n",
    "    \n",
    "    # Add sentiment label\n",
    "    if sentiment['polarity'] >= 0.1:\n",
    "        sentiment['sentiment'] = 'positive'\n",
    "    elif sentiment['polarity'] <= -0.1:\n",
    "        sentiment['sentiment'] = 'negative'\n",
    "    else:\n",
    "        sentiment['sentiment'] = 'neutral'\n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "# Process the dataframe with TextBlob\n",
    "def process_sentiment_textblob(df):\n",
    "    # Create a copy for processing\n",
    "    result_df = df.copy() if isinstance(df, pd.DataFrame) else df.toPandas()\n",
    "    \n",
    "    # Apply sentiment analysis to titles\n",
    "    print(\"Analyzing sentiment of titles with TextBlob...\")\n",
    "    title_sentiments = result_df['title'].apply(analyze_sentiment_textblob)\n",
    "    \n",
    "    # Extract sentiment components\n",
    "    result_df['tb_title_polarity'] = title_sentiments.apply(lambda x: x['polarity'])\n",
    "    result_df['tb_title_subjectivity'] = title_sentiments.apply(lambda x: x['subjectivity'])\n",
    "    result_df['tb_title_sentiment'] = title_sentiments.apply(lambda x: x['sentiment'])\n",
    "    \n",
    "    # Apply sentiment analysis to post content if available\n",
    "    if 'selftext' in result_df.columns:\n",
    "        print(\"Analyzing sentiment of post content with TextBlob...\")\n",
    "        content_sentiments = result_df['selftext'].apply(analyze_sentiment_textblob)\n",
    "        result_df['tb_content_polarity'] = content_sentiments.apply(lambda x: x['polarity'])\n",
    "        result_df['tb_content_subjectivity'] = content_sentiments.apply(lambda x: x['subjectivity'])\n",
    "        result_df['tb_content_sentiment'] = content_sentiments.apply(lambda x: x['sentiment'])\n",
    "    \n",
    "    # Calculate overall sentiment (average of title and content)\n",
    "    if 'tb_content_polarity' in result_df.columns:\n",
    "        result_df['tb_overall_polarity'] = (result_df['tb_title_polarity'] + result_df['tb_content_polarity']) / 2\n",
    "        result_df['tb_overall_subjectivity'] = (result_df['tb_title_subjectivity'] + result_df['tb_content_subjectivity']) / 2\n",
    "        \n",
    "        # Apply sentiment label\n",
    "        result_df['tb_overall_sentiment'] = result_df['tb_overall_polarity'].apply(\n",
    "            lambda x: 'positive' if x >= 0.1 else ('negative' if x <= -0.1 else 'neutral')\n",
    "        )\n",
    "    else:\n",
    "        result_df['tb_overall_polarity'] = result_df['tb_title_polarity']\n",
    "        result_df['tb_overall_subjectivity'] = result_df['tb_title_subjectivity']\n",
    "        result_df['tb_overall_sentiment'] = result_df['tb_title_sentiment']\n",
    "    \n",
    "    print(f\"✅ TextBlob sentiment analysis completed for {len(result_df)} posts\")\n",
    "    \n",
    "    # Convert back to Spark DataFrame if needed\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        spark = create_spark_session()\n",
    "        if spark:\n",
    "            return spark.createDataFrame(result_df)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Apply TextBlob sentiment analysis to our data\n",
    "if 'sentiment_df' in locals() and sentiment_df is not None:\n",
    "    textblob_df = process_sentiment_textblob(sentiment_df)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n--- TextBlob Sentiment Analysis Results ---\")\n",
    "    if isinstance(textblob_df, pd.DataFrame):\n",
    "        display(textblob_df[['title', 'tb_overall_sentiment', 'tb_overall_polarity', 'tb_overall_subjectivity']])\n",
    "        \n",
    "        # Create a chart to visualize sentiment distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sentiment_counts = textblob_df['tb_overall_sentiment'].value_counts()\n",
    "        sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette={'positive': 'green', 'neutral': 'blue', 'negative': 'red'})\n",
    "        plt.title('TextBlob Sentiment Distribution in r/malaysia Posts')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xlabel('Sentiment')\n",
    "        plt.show()\n",
    "    else:\n",
    "        textblob_df.select('title', 'tb_overall_sentiment', 'tb_overall_polarity').show(10, truncate=False)\n",
    "        \n",
    "        # Get counts for visualization\n",
    "        sentiment_counts = textblob_df.groupBy('tb_overall_sentiment').count().toPandas()\n",
    "        \n",
    "        # Create a chart\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='tb_overall_sentiment', y='count', data=sentiment_counts, palette={'positive': 'green', 'neutral': 'blue', 'negative': 'red'})\n",
    "        plt.title('TextBlob Sentiment Distribution in r/malaysia Posts')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xlabel('Sentiment')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"❌ No data available for TextBlob sentiment analysis. Please run the previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f7fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Hugging Face Transformers Sentiment Analysis\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Check if GPU is available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n",
    "\n",
    "# Initialize the sentiment analysis pipeline\n",
    "try:\n",
    "    print(\"Loading sentiment analysis model...\")\n",
    "    # We'll use a smaller model for speed in this notebook environment\n",
    "    sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=device)\n",
    "    print(\"✅ Sentiment analysis model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading sentiment model: {e}\")\n",
    "    print(\"⚠️ Skipping Transformers sentiment analysis\")\n",
    "    sentiment_pipeline = None\n",
    "\n",
    "# Function to analyze sentiment using Transformers\n",
    "def analyze_sentiment_transformers(text, max_length=512):\n",
    "    if not text or pd.isna(text) or text == '' or sentiment_pipeline is None:\n",
    "        return {\n",
    "            'label': 'neutral',\n",
    "            'score': 0.5\n",
    "        }\n",
    "    \n",
    "    # Truncate text if needed (transformers has token limits)\n",
    "    if len(text) > max_length * 4:  # Rough approximation of token count\n",
    "        text = text[:max_length * 4]\n",
    "    \n",
    "    try:\n",
    "        # Get sentiment prediction\n",
    "        result = sentiment_pipeline(text)[0]\n",
    "        \n",
    "        # Map POSITIVE/NEGATIVE to our format\n",
    "        label = result['label'].lower()\n",
    "        score = result['score']\n",
    "        \n",
    "        # Normalize label to match our other models\n",
    "        if label == 'positive' or label == 'POSITIVE':\n",
    "            sentiment = 'positive'\n",
    "        elif label == 'negative' or label == 'NEGATIVE':\n",
    "            sentiment = 'negative'\n",
    "        else:\n",
    "            sentiment = 'neutral'\n",
    "            \n",
    "        return {\n",
    "            'label': sentiment,\n",
    "            'score': score\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in transformer sentiment: {e}\")\n",
    "        return {\n",
    "            'label': 'neutral',\n",
    "            'score': 0.5\n",
    "        }\n",
    "\n",
    "# Process the dataframe with Transformers\n",
    "def process_sentiment_transformers(df):\n",
    "    if sentiment_pipeline is None:\n",
    "        print(\"❌ Transformers model not available, skipping analysis\")\n",
    "        return df\n",
    "    \n",
    "    # Create a copy for processing\n",
    "    result_df = df.copy() if isinstance(df, pd.DataFrame) else df.toPandas()\n",
    "    \n",
    "    # Apply sentiment analysis to titles\n",
    "    print(\"Analyzing sentiment of titles with Transformers...\")\n",
    "    title_sentiments = result_df['title'].apply(analyze_sentiment_transformers)\n",
    "    \n",
    "    # Extract sentiment components\n",
    "    result_df['hf_title_sentiment'] = title_sentiments.apply(lambda x: x['label'])\n",
    "    result_df['hf_title_score'] = title_sentiments.apply(lambda x: x['score'])\n",
    "    \n",
    "    # Apply sentiment analysis to post content if available and it's not too much data\n",
    "    # For demonstration purposes, only analyze content for a few posts to save time\n",
    "    sample_size = min(5, len(result_df))\n",
    "    if 'selftext' in result_df.columns:\n",
    "        print(f\"Analyzing sentiment of content for {sample_size} posts with Transformers (sample only)...\")\n",
    "        \n",
    "        # Only process a sample of posts to save time\n",
    "        sample_indices = result_df.index[:sample_size]\n",
    "        \n",
    "        for idx in sample_indices:\n",
    "            text = result_df.loc[idx, 'selftext']\n",
    "            if text and not pd.isna(text) and text != '':\n",
    "                sentiment = analyze_sentiment_transformers(text)\n",
    "                result_df.loc[idx, 'hf_content_sentiment'] = sentiment['label']\n",
    "                result_df.loc[idx, 'hf_content_score'] = sentiment['score']\n",
    "    \n",
    "    print(f\"✅ Transformers sentiment analysis completed\")\n",
    "    \n",
    "    # Convert back to Spark DataFrame if needed\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        spark = create_spark_session()\n",
    "        if spark:\n",
    "            return spark.createDataFrame(result_df)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Apply Transformers sentiment analysis to our data\n",
    "if 'textblob_df' in locals() and textblob_df is not None:\n",
    "    # This could take some time depending on the hardware\n",
    "    transformers_df = process_sentiment_transformers(textblob_df)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n--- Hugging Face Transformers Sentiment Analysis Results ---\")\n",
    "    if isinstance(transformers_df, pd.DataFrame):\n",
    "        display(transformers_df[['title', 'hf_title_sentiment', 'hf_title_score']])\n",
    "        \n",
    "        # Create a chart to visualize sentiment distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sentiment_counts = transformers_df['hf_title_sentiment'].value_counts()\n",
    "        sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette={'positive': 'green', 'neutral': 'blue', 'negative': 'red'})\n",
    "        plt.title('Transformers Sentiment Distribution in r/malaysia Posts')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xlabel('Sentiment')\n",
    "        plt.show()\n",
    "    else:\n",
    "        transformers_df.select('title', 'hf_title_sentiment', 'hf_title_score').show(10, truncate=False)\n",
    "else:\n",
    "    print(\"❌ No data available for Transformers sentiment analysis. Please run the previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d4e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Compare Sentiment Analysis Models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to compare different sentiment models\n",
    "def compare_sentiment_models(df):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        df = df.toPandas()\n",
    "    \n",
    "    # Create a comparison dataframe\n",
    "    comparison_df = pd.DataFrame()\n",
    "    \n",
    "    # Check which models we have data for\n",
    "    models = []\n",
    "    \n",
    "    if 'overall_sentiment' in df.columns:\n",
    "        comparison_df['VADER'] = df['overall_sentiment']\n",
    "        models.append('VADER')\n",
    "    \n",
    "    if 'tb_overall_sentiment' in df.columns:\n",
    "        comparison_df['TextBlob'] = df['tb_overall_sentiment']\n",
    "        models.append('TextBlob')\n",
    "    \n",
    "    if 'hf_title_sentiment' in df.columns:\n",
    "        comparison_df['Transformers'] = df['hf_title_sentiment']\n",
    "        models.append('Transformers')\n",
    "    \n",
    "    if len(models) < 2:\n",
    "        print(\"❌ Need at least 2 models to compare\")\n",
    "        return\n",
    "    \n",
    "    # Add titles for reference\n",
    "    comparison_df['title'] = df['title']\n",
    "    \n",
    "    # Display the comparison\n",
    "    print(\"--- Sentiment Analysis Model Comparison ---\")\n",
    "    display(comparison_df[['title'] + models])\n",
    "    \n",
    "    # Calculate agreement between models\n",
    "    print(\"\\n--- Model Agreement Analysis ---\")\n",
    "    \n",
    "    # Pairwise comparison\n",
    "    for i in range(len(models)):\n",
    "        for j in range(i + 1, len(models)):\n",
    "            model1 = models[i]\n",
    "            model2 = models[j]\n",
    "            agreement = (comparison_df[model1] == comparison_df[model2]).mean() * 100\n",
    "            print(f\"Agreement between {model1} and {model2}: {agreement:.1f}%\")\n",
    "    \n",
    "    # Create a stacked bar chart to compare sentiment distributions\n",
    "    sentiment_counts = {}\n",
    "    \n",
    "    for model in models:\n",
    "        sentiment_counts[model] = comparison_df[model].value_counts().reindex(['positive', 'neutral', 'negative']).fillna(0)\n",
    "    \n",
    "    # Plot the results\n",
    "    fig, axes = plt.subplots(1, len(models), figsize=(15, 6), sharey=True)\n",
    "    \n",
    "    if len(models) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        counts = sentiment_counts[model]\n",
    "        sns.barplot(x=counts.index, y=counts.values, ax=axes[i], \n",
    "                    palette={'positive': 'green', 'neutral': 'blue', 'negative': 'red'})\n",
    "        axes[i].set_title(f'{model} Sentiment')\n",
    "        axes[i].set_ylim(0, max([counts.max() for counts in sentiment_counts.values()]) * 1.1)\n",
    "        \n",
    "        # Add percentage labels\n",
    "        total = counts.sum()\n",
    "        for j, p in enumerate(axes[i].patches):\n",
    "            percentage = f'{100 * p.get_height() / total:.1f}%'\n",
    "            axes[i].annotate(percentage, (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                            ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Compare our models if we have them\n",
    "if 'transformers_df' in locals() and transformers_df is not None:\n",
    "    comparison_results = compare_sentiment_models(transformers_df)\n",
    "elif 'textblob_df' in locals() and textblob_df is not None:\n",
    "    comparison_results = compare_sentiment_models(textblob_df)\n",
    "elif 'sentiment_df' in locals() and sentiment_df is not None:\n",
    "    comparison_results = compare_sentiment_models(sentiment_df)\n",
    "else:\n",
    "    print(\"❌ No sentiment analysis results available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa14e30",
   "metadata": {},
   "source": [
    "## 4. Elasticsearch Integration\n",
    "\n",
    "In this section, we'll store our analyzed data in Elasticsearch for real-time querying and visualization. Elasticsearch is an excellent choice for this project because:\n",
    "\n",
    "1. It provides fast, real-time search and analytics\n",
    "2. It scales horizontally for large data volumes\n",
    "3. It integrates well with Kibana for dashboards and visualization\n",
    "4. It supports full-text search with linguistic analysis\n",
    "\n",
    "For a production environment, you would need:\n",
    "- Elasticsearch server (standalone or cloud)\n",
    "- Kibana for visualization\n",
    "- Logstash or direct integration for data ingestion\n",
    "\n",
    "For this notebook, we'll demonstrate how to:\n",
    "1. Connect to Elasticsearch\n",
    "2. Create an index with proper mappings for our data\n",
    "3. Index our sentiment analysis results\n",
    "4. Perform basic queries to analyze the data\n",
    "\n",
    "### Implementation Plan\n",
    "1. Set up a connection to Elasticsearch\n",
    "2. Define the index mappings for Reddit posts with sentiment analysis\n",
    "3. Index our analyzed data\n",
    "4. Perform queries to extract insights about Malaysian sentiment\n",
    "5. Export data for our Streamlit dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf6b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Setting up Elasticsearch Connection\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch.exceptions import NotFoundError\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Function to create Elasticsearch client\n",
    "def create_elasticsearch_client():\n",
    "    try:\n",
    "        # Get credentials from env\n",
    "        es_host = os.environ.get('ES_HOST', 'localhost')\n",
    "        es_port = os.environ.get('ES_PORT', '9200')\n",
    "        \n",
    "        # Create connection\n",
    "        es = Elasticsearch([f\"http://{es_host}:{es_port}\"], timeout=30)\n",
    "        \n",
    "        # Test connection\n",
    "        if es.ping():\n",
    "            print(f\"✅ Connected to Elasticsearch at {es_host}:{es_port}\")\n",
    "            print(f\"Elasticsearch info: {es.info().body}\")\n",
    "            return es\n",
    "        else:\n",
    "            print(\"❌ Could not connect to Elasticsearch\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error connecting to Elasticsearch: {e}\")\n",
    "        print(\"⚠️ Using simulated Elasticsearch environment\")\n",
    "        return None\n",
    "\n",
    "# Create a simulated client for development if real ES is not available\n",
    "class SimulatedElasticsearch:\n",
    "    def __init__(self):\n",
    "        self.indices = {}\n",
    "        self.documents = {}\n",
    "        print(\"✓ Initialized simulated Elasticsearch client\")\n",
    "    \n",
    "    def ping(self):\n",
    "        return True\n",
    "    \n",
    "    def info(self):\n",
    "        class Body:\n",
    "            def __init__(self):\n",
    "                self.body = {\n",
    "                    \"version\": {\n",
    "                        \"number\": \"8.11.0-simulated\"\n",
    "                    },\n",
    "                    \"name\": \"simulated-elasticsearch\"\n",
    "                }\n",
    "        return Body()\n",
    "    \n",
    "    def indices(self):\n",
    "        class Indices:\n",
    "            def create(self, index, body=None):\n",
    "                print(f\"✓ Created simulated index: {index}\")\n",
    "                return {\"acknowledged\": True}\n",
    "            \n",
    "            def exists(self, index):\n",
    "                return index in self.indices\n",
    "            \n",
    "            def delete(self, index):\n",
    "                print(f\"✓ Deleted simulated index: {index}\")\n",
    "                return {\"acknowledged\": True}\n",
    "        return Indices()\n",
    "    \n",
    "    def index(self, index, id=None, document=None, **kwargs):\n",
    "        if index not in self.documents:\n",
    "            self.documents[index] = {}\n",
    "        \n",
    "        if id is None:\n",
    "            id = str(len(self.documents[index]) + 1)\n",
    "            \n",
    "        self.documents[index][id] = document\n",
    "        return {\"result\": \"created\", \"_id\": id}\n",
    "    \n",
    "    def get(self, index, id):\n",
    "        if index in self.documents and id in self.documents[index]:\n",
    "            return {\"_source\": self.documents[index][id], \"found\": True}\n",
    "        raise NotFoundError(f\"Document not found: {index}/{id}\")\n",
    "    \n",
    "    def search(self, index=None, body=None):\n",
    "        hits = []\n",
    "        \n",
    "        if index in self.documents:\n",
    "            for id, doc in self.documents[index].items():\n",
    "                hits.append({\"_id\": id, \"_source\": doc})\n",
    "        \n",
    "        return {\n",
    "            \"hits\": {\n",
    "                \"total\": {\"value\": len(hits)},\n",
    "                \"hits\": hits\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def count(self, index=None, body=None):\n",
    "        count = 0\n",
    "        if index in self.documents:\n",
    "            count = len(self.documents[index])\n",
    "        return {\"count\": count}\n",
    "\n",
    "# Connect to Elasticsearch or use simulated client\n",
    "es_client = create_elasticsearch_client()\n",
    "\n",
    "if es_client is None:\n",
    "    print(\"⚠️ Using simulated Elasticsearch client\")\n",
    "    es_client = SimulatedElasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Creating the Elasticsearch Index for Reddit Posts\n",
    "def create_reddit_index(es, index_name=\"reddit_malaysia_sentiment\"):\n",
    "    # Define the mapping for our index\n",
    "    mappings = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"id\": {\"type\": \"keyword\"},\n",
    "                \"title\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "                \"selftext\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "                \"author\": {\"type\": \"keyword\"},\n",
    "                \"created_utc\": {\"type\": \"date\"},\n",
    "                \"score\": {\"type\": \"integer\"},\n",
    "                \"num_comments\": {\"type\": \"integer\"},\n",
    "                \"url\": {\"type\": \"keyword\"},\n",
    "                \n",
    "                # VADER sentiment fields\n",
    "                \"title_sentiment\": {\"type\": \"keyword\"},\n",
    "                \"title_compound\": {\"type\": \"float\"},\n",
    "                \"content_sentiment\": {\"type\": \"keyword\"},\n",
    "                \"content_compound\": {\"type\": \"float\"},\n",
    "                \"overall_sentiment\": {\"type\": \"keyword\"},\n",
    "                \"overall_compound\": {\"type\": \"float\"},\n",
    "                \n",
    "                # TextBlob sentiment fields\n",
    "                \"tb_title_sentiment\": {\"type\": \"keyword\"},\n",
    "                \"tb_title_polarity\": {\"type\": \"float\"},\n",
    "                \"tb_title_subjectivity\": {\"type\": \"float\"},\n",
    "                \"tb_content_sentiment\": {\"type\": \"keyword\"},\n",
    "                \"tb_content_polarity\": {\"type\": \"float\"},\n",
    "                \"tb_content_subjectivity\": {\"type\": \"float\"},\n",
    "                \"tb_overall_sentiment\": {\"type\": \"keyword\"},\n",
    "                \"tb_overall_polarity\": {\"type\": \"float\"},\n",
    "                \"tb_overall_subjectivity\": {\"type\": \"float\"},\n",
    "                \n",
    "                # Transformers sentiment fields\n",
    "                \"hf_title_sentiment\": {\"type\": \"keyword\"},\n",
    "                \"hf_title_score\": {\"type\": \"float\"},\n",
    "                \"hf_content_sentiment\": {\"type\": \"keyword\"},\n",
    "                \"hf_content_score\": {\"type\": \"float\"},\n",
    "                \n",
    "                # Timestamps and metadata\n",
    "                \"indexed_at\": {\"type\": \"date\"},\n",
    "                \"data_source\": {\"type\": \"keyword\"},\n",
    "                \"processed_by\": {\"type\": \"keyword\"}\n",
    "            }\n",
    "        },\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check if index exists and delete if it does\n",
    "        try:\n",
    "            if isinstance(es, Elasticsearch) and es.indices.exists(index=index_name):\n",
    "                es.indices.delete(index=index_name)\n",
    "                print(f\"✓ Deleted existing index: {index_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Note: {e}\")\n",
    "        \n",
    "        # Create the index with our mappings\n",
    "        if isinstance(es, Elasticsearch):\n",
    "            es.indices.create(index=index_name, body=mappings)\n",
    "        else:\n",
    "            # Simulated client\n",
    "            es.indices().create(index=index_name, body=mappings)\n",
    "            \n",
    "        print(f\"✅ Created index: {index_name} with proper mappings\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating index: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create our index\n",
    "index_name = \"reddit_malaysia_sentiment\"\n",
    "create_reddit_index(es_client, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf574b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Indexing Sentiment Analysis Results to Elasticsearch\n",
    "def index_sentiment_analysis_results(es, df, index_name=\"reddit_malaysia_sentiment\"):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        df = df.toPandas()\n",
    "    \n",
    "    # Count of documents to be indexed\n",
    "    doc_count = len(df)\n",
    "    \n",
    "    if doc_count == 0:\n",
    "        print(\"❌ No data to index\")\n",
    "        return 0\n",
    "    \n",
    "    print(f\"🔄 Indexing {doc_count} documents to Elasticsearch...\")\n",
    "    \n",
    "    # Prepare documents for bulk indexing\n",
    "    documents = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Create base document with common fields\n",
    "        doc = {\n",
    "            \"id\": row.get('id', ''),\n",
    "            \"title\": row.get('title', ''),\n",
    "            \"author\": row.get('author', ''),\n",
    "            \"score\": row.get('score', 0),\n",
    "            \"num_comments\": row.get('num_comments', 0),\n",
    "            \"url\": row.get('url', ''),\n",
    "            \"created_utc\": datetime.fromtimestamp(row.get('created_utc', time.time())).isoformat(),\n",
    "            \"indexed_at\": datetime.now().isoformat(),\n",
    "            \"data_source\": \"r/malaysia\",\n",
    "            \"processed_by\": \"sentiment_analysis_pipeline\"\n",
    "        }\n",
    "        \n",
    "        # Add selftext if available\n",
    "        if 'selftext' in row and not pd.isna(row['selftext']):\n",
    "            doc[\"selftext\"] = row['selftext']\n",
    "        \n",
    "        # Add VADER sentiment fields if available\n",
    "        for field in ['title_sentiment', 'title_compound', 'content_sentiment', \n",
    "                     'content_compound', 'overall_sentiment', 'overall_compound']:\n",
    "            if field in row and not pd.isna(row[field]):\n",
    "                doc[field] = row[field]\n",
    "        \n",
    "        # Add TextBlob sentiment fields if available\n",
    "        for field in ['tb_title_sentiment', 'tb_title_polarity', 'tb_title_subjectivity',\n",
    "                     'tb_content_sentiment', 'tb_content_polarity', 'tb_content_subjectivity',\n",
    "                     'tb_overall_sentiment', 'tb_overall_polarity', 'tb_overall_subjectivity']:\n",
    "            if field in row and not pd.isna(row[field]):\n",
    "                doc[field] = row[field]\n",
    "        \n",
    "        # Add Transformers sentiment fields if available\n",
    "        for field in ['hf_title_sentiment', 'hf_title_score', \n",
    "                     'hf_content_sentiment', 'hf_content_score']:\n",
    "            if field in row and not pd.isna(row[field]):\n",
    "                doc[field] = row[field]\n",
    "        \n",
    "        documents.append(doc)\n",
    "    \n",
    "    # Index the documents\n",
    "    successful = 0\n",
    "    \n",
    "    if isinstance(es, Elasticsearch):\n",
    "        # For real Elasticsearch, use bulk indexing\n",
    "        try:\n",
    "            actions = [\n",
    "                {\n",
    "                    \"_index\": index_name,\n",
    "                    \"_id\": doc[\"id\"],\n",
    "                    \"_source\": doc\n",
    "                }\n",
    "                for doc in documents\n",
    "            ]\n",
    "            \n",
    "            # Bulk index\n",
    "            success, failed = helpers.bulk(es, actions, stats_only=True)\n",
    "            print(f\"✅ Successfully indexed {success} documents\")\n",
    "            if failed:\n",
    "                print(f\"❌ Failed to index {failed} documents\")\n",
    "            \n",
    "            successful = success\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during bulk indexing: {e}\")\n",
    "    else:\n",
    "        # For simulated Elasticsearch\n",
    "        for doc in documents:\n",
    "            try:\n",
    "                res = es.index(index=index_name, id=doc[\"id\"], document=doc)\n",
    "                if res[\"result\"] == \"created\":\n",
    "                    successful += 1\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error indexing document: {e}\")\n",
    "        \n",
    "        print(f\"✅ Successfully indexed {successful} documents in simulated environment\")\n",
    "    \n",
    "    return successful\n",
    "\n",
    "# Index our sentiment analysis results if available\n",
    "if 'transformers_df' in locals() and transformers_df is not None:\n",
    "    indexed_count = index_sentiment_analysis_results(es_client, transformers_df, index_name)\n",
    "elif 'textblob_df' in locals() and textblob_df is not None:\n",
    "    indexed_count = index_sentiment_analysis_results(es_client, textblob_df, index_name)\n",
    "elif 'sentiment_df' in locals() and sentiment_df is not None:\n",
    "    indexed_count = index_sentiment_analysis_results(es_client, sentiment_df, index_name)\n",
    "else:\n",
    "    print(\"❌ No sentiment analysis results available to index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae7e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 Querying Elasticsearch for Sentiment Analysis Insights\n",
    "def query_elasticsearch_sentiment(es, index_name=\"reddit_malaysia_sentiment\"):\n",
    "    print(\"🔍 Querying Elasticsearch for sentiment insights...\")\n",
    "    \n",
    "    # Define our queries\n",
    "    queries = [\n",
    "        {\n",
    "            \"name\": \"Overall Sentiment Distribution (VADER)\",\n",
    "            \"query\": {\n",
    "                \"size\": 0,\n",
    "                \"aggs\": {\n",
    "                    \"sentiment_distribution\": {\n",
    "                        \"terms\": {\n",
    "                            \"field\": \"overall_sentiment\",\n",
    "                            \"size\": 10\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Overall Sentiment Distribution (TextBlob)\",\n",
    "            \"query\": {\n",
    "                \"size\": 0,\n",
    "                \"aggs\": {\n",
    "                    \"sentiment_distribution\": {\n",
    "                        \"terms\": {\n",
    "                            \"field\": \"tb_overall_sentiment\",\n",
    "                            \"size\": 10\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Top Positive Posts (VADER)\",\n",
    "            \"query\": {\n",
    "                \"size\": 5,\n",
    "                \"sort\": [{\"overall_compound\": {\"order\": \"desc\"}}],\n",
    "                \"query\": {\n",
    "                    \"term\": {\"overall_sentiment\": \"positive\"}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Top Negative Posts (VADER)\",\n",
    "            \"query\": {\n",
    "                \"size\": 5,\n",
    "                \"sort\": [{\"overall_compound\": {\"order\": \"asc\"}}],\n",
    "                \"query\": {\n",
    "                    \"term\": {\"overall_sentiment\": \"negative\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Execute queries\n",
    "    for query_spec in queries:\n",
    "        query_name = query_spec[\"name\"]\n",
    "        query_body = query_spec[\"query\"]\n",
    "        \n",
    "        print(f\"\\n--- {query_name} ---\")\n",
    "        \n",
    "        try:\n",
    "            if isinstance(es, Elasticsearch):\n",
    "                result = es.search(index=index_name, body=query_body)\n",
    "            else:\n",
    "                # For simulated Elasticsearch\n",
    "                result = es.search(index=index_name, body=query_body)\n",
    "                \n",
    "            results[query_name] = result\n",
    "            \n",
    "            # Print results based on query type\n",
    "            if \"aggs\" in query_body:\n",
    "                # Aggregation query\n",
    "                if \"sentiment_distribution\" in result.get(\"aggregations\", {}):\n",
    "                    buckets = result[\"aggregations\"][\"sentiment_distribution\"][\"buckets\"]\n",
    "                    for bucket in buckets:\n",
    "                        print(f\"{bucket['key']}: {bucket['doc_count']} posts\")\n",
    "            else:\n",
    "                # Document query\n",
    "                hits = result.get(\"hits\", {}).get(\"hits\", [])\n",
    "                for hit in hits:\n",
    "                    source = hit.get(\"_source\", {})\n",
    "                    print(f\"Title: {source.get('title', 'N/A')}\")\n",
    "                    print(f\"Score: {source.get('score', 'N/A')}\")\n",
    "                    print(f\"Sentiment: {source.get('overall_sentiment', 'N/A')} (compound: {source.get('overall_compound', 'N/A'):.3f})\")\n",
    "                    print(\"---\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error executing query: {e}\")\n",
    "            results[query_name] = {\"error\": str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Execute sentiment analysis queries\n",
    "if 'es_client' in locals() and es_client is not None:\n",
    "    sentiment_insights = query_elasticsearch_sentiment(es_client, index_name)\n",
    "else:\n",
    "    print(\"❌ No Elasticsearch client available. Please run the previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aee9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 Exporting Data for Streamlit Dashboard\n",
    "def export_data_for_dashboard(df, filename=\"malaysia_reddit_sentiment.csv\"):\n",
    "    \"\"\"Export the sentiment analysis data for use in the Streamlit dashboard\"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        df = df.toPandas()\n",
    "    \n",
    "    # Select the most important columns for the dashboard\n",
    "    dashboard_columns = [\n",
    "        # Post metadata\n",
    "        'id', 'title', 'author', 'created_utc', 'score', 'num_comments', 'url',\n",
    "        \n",
    "        # VADER sentiment\n",
    "        'overall_sentiment', 'overall_compound',\n",
    "        \n",
    "        # TextBlob sentiment\n",
    "        'tb_overall_sentiment', 'tb_overall_polarity', 'tb_overall_subjectivity',\n",
    "        \n",
    "        # Transformers sentiment (if available)\n",
    "        'hf_title_sentiment', 'hf_title_score'\n",
    "    ]\n",
    "    \n",
    "    # Filter to available columns\n",
    "    available_columns = [col for col in dashboard_columns if col in df.columns]\n",
    "    export_df = df[available_columns].copy()\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    if 'created_utc' in export_df.columns:\n",
    "        export_df['created_utc'] = pd.to_datetime(export_df['created_utc'], unit='s')\n",
    "    \n",
    "    # Save to CSV\n",
    "    try:\n",
    "        export_df.to_csv(filename, index=False)\n",
    "        print(f\"✅ Successfully exported data to {filename}\")\n",
    "        print(f\"   Rows: {len(export_df)}\")\n",
    "        print(f\"   Columns: {', '.join(export_df.columns)}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error exporting data: {e}\")\n",
    "        return False\n",
    "\n",
    "# Export data if available\n",
    "if 'transformers_df' in locals() and transformers_df is not None:\n",
    "    export_success = export_data_for_dashboard(transformers_df)\n",
    "elif 'textblob_df' in locals() and textblob_df is not None:\n",
    "    export_success = export_data_for_dashboard(textblob_df)\n",
    "elif 'sentiment_df' in locals() and sentiment_df is not None:\n",
    "    export_success = export_data_for_dashboard(sentiment_df)\n",
    "else:\n",
    "    print(\"❌ No sentiment analysis results available to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c144d",
   "metadata": {},
   "source": [
    "## 5. Streamlit Dashboard for Sentiment Visualization\n",
    "\n",
    "Now we'll create an interactive Streamlit dashboard to visualize our sentiment analysis results. The dashboard will include:\n",
    "\n",
    "1. **Overview Statistics**: Key metrics about our Reddit data\n",
    "2. **Sentiment Distribution**: Visual comparison of sentiment across models\n",
    "3. **Time Series Analysis**: Sentiment trends over time\n",
    "4. **Top Posts**: Most positive and negative posts\n",
    "5. **Sentiment by Topic**: Using NLP techniques to identify key topics in r/malaysia\n",
    "\n",
    "This Streamlit app will provide an intuitive interface for exploring the Malaysian public sentiment as expressed on r/malaysia.\n",
    "\n",
    "### Implementation Plan\n",
    "1. Create a Streamlit application structure\n",
    "2. Implement data loading and preprocessing\n",
    "3. Build interactive visualizations\n",
    "4. Add filters for time period, sentiment model, and topics\n",
    "5. Deploy the dashboard locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e9a584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Creating the Streamlit App\n",
    "%%writefile malaysia_reddit_sentiment_dashboard.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK data\n",
    "@st.cache_resource\n",
    "def download_nltk_data():\n",
    "    try:\n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('punkt')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "download_nltk_data()\n",
    "\n",
    "# Set page config\n",
    "st.set_page_config(\n",
    "    page_title=\"r/Malaysia Sentiment Dashboard\",\n",
    "    page_icon=\"🇲🇾\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# Function to load data\n",
    "@st.cache_data\n",
    "def load_data(file_path='malaysia_reddit_sentiment.csv'):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Convert date columns\n",
    "        if 'created_utc' in df.columns:\n",
    "            df['created_utc'] = pd.to_datetime(df['created_utc'])\n",
    "            df['date'] = df['created_utc'].dt.date\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main app\n",
    "def main():\n",
    "    # Header\n",
    "    st.title(\"🇲🇾 Malaysian Reddit Sentiment Analysis\")\n",
    "    st.markdown(\"### Real-time sentiment analysis of r/malaysia subreddit\")\n",
    "    \n",
    "    # Load data\n",
    "    data = load_data()\n",
    "    \n",
    "    if data is None or len(data) == 0:\n",
    "        st.warning(\"No data available. Please run the notebook to generate the sentiment analysis data.\")\n",
    "        st.stop()\n",
    "    \n",
    "    # Sidebar\n",
    "    st.sidebar.title(\"Dashboard Controls\")\n",
    "    \n",
    "    # Date filter\n",
    "    st.sidebar.subheader(\"Time Period\")\n",
    "    \n",
    "    # Get min and max dates\n",
    "    min_date = data['created_utc'].min().date() if 'created_utc' in data.columns else datetime.now().date() - timedelta(days=30)\n",
    "    max_date = data['created_utc'].max().date() if 'created_utc' in data.columns else datetime.now().date()\n",
    "    \n",
    "    start_date = st.sidebar.date_input(\"Start Date\", min_date)\n",
    "    end_date = st.sidebar.date_input(\"End Date\", max_date)\n",
    "    \n",
    "    # Model selection\n",
    "    st.sidebar.subheader(\"Sentiment Model\")\n",
    "    model_options = []\n",
    "    \n",
    "    if 'overall_sentiment' in data.columns:\n",
    "        model_options.append('VADER')\n",
    "    if 'tb_overall_sentiment' in data.columns:\n",
    "        model_options.append('TextBlob')\n",
    "    if 'hf_title_sentiment' in data.columns:\n",
    "        model_options.append('HuggingFace')\n",
    "    \n",
    "    selected_model = st.sidebar.selectbox(\"Select Model\", model_options, index=0 if model_options else None)\n",
    "    \n",
    "    # Filter data based on selection\n",
    "    if 'date' in data.columns:\n",
    "        filtered_data = data[(data['date'] >= start_date) & (data['date'] <= end_date)]\n",
    "    else:\n",
    "        filtered_data = data\n",
    "    \n",
    "    # Overview stats\n",
    "    st.header(\"📊 Dashboard Overview\")\n",
    "    \n",
    "    col1, col2, col3, col4 = st.columns(4)\n",
    "    \n",
    "    with col1:\n",
    "        st.metric(\"Total Posts\", len(filtered_data))\n",
    "    \n",
    "    with col2:\n",
    "        avg_score = int(filtered_data['score'].mean()) if 'score' in filtered_data.columns else 'N/A'\n",
    "        st.metric(\"Average Score\", avg_score)\n",
    "    \n",
    "    with col3:\n",
    "        avg_comments = int(filtered_data['num_comments'].mean()) if 'num_comments' in filtered_data.columns else 'N/A'\n",
    "        st.metric(\"Average Comments\", avg_comments)\n",
    "    \n",
    "    with col4:\n",
    "        # Get the sentiment field based on selected model\n",
    "        if selected_model == 'VADER' and 'overall_sentiment' in filtered_data.columns:\n",
    "            sentiment_field = 'overall_sentiment'\n",
    "        elif selected_model == 'TextBlob' and 'tb_overall_sentiment' in filtered_data.columns:\n",
    "            sentiment_field = 'tb_overall_sentiment'\n",
    "        elif selected_model == 'HuggingFace' and 'hf_title_sentiment' in filtered_data.columns:\n",
    "            sentiment_field = 'hf_title_sentiment'\n",
    "        else:\n",
    "            sentiment_field = None\n",
    "        \n",
    "        if sentiment_field and sentiment_field in filtered_data.columns:\n",
    "            positive_pct = filtered_data[sentiment_field].value_counts(normalize=True).get('positive', 0) * 100\n",
    "            st.metric(\"Positive Sentiment\", f\"{positive_pct:.1f}%\")\n",
    "    \n",
    "    # Sentiment Distribution\n",
    "    st.header(\"😀😐😠 Sentiment Distribution\")\n",
    "    \n",
    "    # Get the appropriate sentiment field and score field based on model\n",
    "    if selected_model == 'VADER':\n",
    "        sentiment_field = 'overall_sentiment'\n",
    "        score_field = 'overall_compound'\n",
    "    elif selected_model == 'TextBlob':\n",
    "        sentiment_field = 'tb_overall_sentiment'\n",
    "        score_field = 'tb_overall_polarity'\n",
    "    elif selected_model == 'HuggingFace':\n",
    "        sentiment_field = 'hf_title_sentiment'\n",
    "        score_field = 'hf_title_score'\n",
    "    else:\n",
    "        sentiment_field = None\n",
    "        score_field = None\n",
    "    \n",
    "    if sentiment_field and sentiment_field in filtered_data.columns:\n",
    "        col1, col2 = st.columns(2)\n",
    "        \n",
    "        with col1:\n",
    "            # Sentiment distribution bar chart\n",
    "            sentiment_counts = filtered_data[sentiment_field].value_counts().reset_index()\n",
    "            sentiment_counts.columns = ['Sentiment', 'Count']\n",
    "            \n",
    "            # Use Plotly for interactive chart\n",
    "            fig = px.bar(\n",
    "                sentiment_counts, \n",
    "                x='Sentiment', \n",
    "                y='Count',\n",
    "                color='Sentiment',\n",
    "                color_discrete_map={'positive': 'green', 'neutral': 'blue', 'negative': 'red'},\n",
    "                title=f\"Sentiment Distribution ({selected_model} model)\"\n",
    "            )\n",
    "            \n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        with col2:\n",
    "            # Score distribution histogram\n",
    "            if score_field and score_field in filtered_data.columns:\n",
    "                fig = px.histogram(\n",
    "                    filtered_data, \n",
    "                    x=score_field,\n",
    "                    nbins=30,\n",
    "                    color_discrete_sequence=['skyblue'],\n",
    "                    title=f\"Sentiment Score Distribution ({selected_model} model)\"\n",
    "                )\n",
    "                \n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "    else:\n",
    "        st.warning(f\"No sentiment data available for {selected_model} model.\")\n",
    "    \n",
    "    # Time series analysis (if date column exists)\n",
    "    if 'created_utc' in filtered_data.columns and sentiment_field in filtered_data.columns:\n",
    "        st.header(\"📈 Sentiment Over Time\")\n",
    "        \n",
    "        # Group by date and calculate average sentiment\n",
    "        filtered_data['date'] = filtered_data['created_utc'].dt.date\n",
    "        \n",
    "        # For VADER and TextBlob, we have numeric scores\n",
    "        if selected_model in ['VADER', 'TextBlob']:\n",
    "            score_field = 'overall_compound' if selected_model == 'VADER' else 'tb_overall_polarity'\n",
    "            \n",
    "            if score_field in filtered_data.columns:\n",
    "                time_data = filtered_data.groupby('date')[score_field].mean().reset_index()\n",
    "                time_data.columns = ['Date', 'Average Sentiment Score']\n",
    "                \n",
    "                fig = px.line(\n",
    "                    time_data, \n",
    "                    x='Date', \n",
    "                    y='Average Sentiment Score',\n",
    "                    title=f\"Average Sentiment Score Over Time ({selected_model})\",\n",
    "                    markers=True\n",
    "                )\n",
    "                \n",
    "                # Add horizontal line at neutral sentiment\n",
    "                fig.add_shape(\n",
    "                    type=\"line\",\n",
    "                    x0=time_data['Date'].min(),\n",
    "                    y0=0,\n",
    "                    x1=time_data['Date'].max(),\n",
    "                    y1=0,\n",
    "                    line=dict(color=\"gray\", width=1, dash=\"dash\")\n",
    "                )\n",
    "                \n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        # For all models, show count by sentiment category over time\n",
    "        sentiment_over_time = filtered_data.groupby(['date', sentiment_field]).size().reset_index()\n",
    "        sentiment_over_time.columns = ['Date', 'Sentiment', 'Count']\n",
    "        \n",
    "        fig = px.line(\n",
    "            sentiment_over_time,\n",
    "            x='Date',\n",
    "            y='Count',\n",
    "            color='Sentiment',\n",
    "            title=f\"Sentiment Trends Over Time ({selected_model})\",\n",
    "            color_discrete_map={'positive': 'green', 'neutral': 'blue', 'negative': 'red'},\n",
    "            markers=True\n",
    "        )\n",
    "        \n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    # Top posts by sentiment\n",
    "    st.header(\"🔍 Top Posts by Sentiment\")\n",
    "    \n",
    "    col1, col2 = st.columns(2)\n",
    "    \n",
    "    with col1:\n",
    "        st.subheader(\"Most Positive Posts\")\n",
    "        \n",
    "        if sentiment_field and score_field and sentiment_field in filtered_data.columns and score_field in filtered_data.columns:\n",
    "            # Sort by sentiment score (descending)\n",
    "            positive_df = filtered_data[filtered_data[sentiment_field] == 'positive'].sort_values(by=score_field, ascending=False).head(5)\n",
    "            \n",
    "            for i, row in positive_df.iterrows():\n",
    "                score_value = row[score_field]\n",
    "                \n",
    "                st.markdown(f\"**{row['title']}**\")\n",
    "                st.markdown(f\"Score: {row['score']} | Comments: {row['num_comments']} | Sentiment: {score_value:.3f}\")\n",
    "                st.markdown(\"---\")\n",
    "    \n",
    "    with col2:\n",
    "        st.subheader(\"Most Negative Posts\")\n",
    "        \n",
    "        if sentiment_field and score_field and sentiment_field in filtered_data.columns and score_field in filtered_data.columns:\n",
    "            # Sort by sentiment score (ascending)\n",
    "            negative_df = filtered_data[filtered_data[sentiment_field] == 'negative'].sort_values(by=score_field, ascending=True).head(5)\n",
    "            \n",
    "            for i, row in negative_df.iterrows():\n",
    "                score_value = row[score_field]\n",
    "                \n",
    "                st.markdown(f\"**{row['title']}**\")\n",
    "                st.markdown(f\"Score: {row['score']} | Comments: {row['num_comments']} | Sentiment: {score_value:.3f}\")\n",
    "                st.markdown(\"---\")\n",
    "    \n",
    "    # Word cloud for topics\n",
    "    st.header(\"🔤 Common Topics in r/malaysia\")\n",
    "    \n",
    "    # Combine title text for word cloud\n",
    "    if 'title' in filtered_data.columns:\n",
    "        all_titles = \" \".join(filtered_data['title'].fillna(\"\"))\n",
    "        \n",
    "        # Process text for word cloud\n",
    "        def process_text(text):\n",
    "            # Tokenize\n",
    "            tokens = word_tokenize(text.lower())\n",
    "            \n",
    "            # Remove stopwords and short words\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            custom_stops = {'r', 'malaysia', 'malaysian', 'anyone', 'like', 'just', 'will', 'can', 'may', 'one', 'get', 'know'}\n",
    "            stop_words.update(custom_stops)\n",
    "            \n",
    "            filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words and len(word) > 2]\n",
    "            \n",
    "            return \" \".join(filtered_tokens)\n",
    "        \n",
    "        processed_text = process_text(all_titles)\n",
    "        \n",
    "        # Generate and display word cloud\n",
    "        if processed_text:\n",
    "            wordcloud = WordCloud(\n",
    "                width=800, \n",
    "                height=400, \n",
    "                background_color='white',\n",
    "                colormap='viridis', \n",
    "                max_words=100\n",
    "            ).generate(processed_text)\n",
    "            \n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            st.pyplot(plt)\n",
    "            \n",
    "            # Show top topics as a bar chart\n",
    "            word_counts = Counter(processed_text.split())\n",
    "            top_words = pd.DataFrame(word_counts.most_common(15), columns=['Word', 'Frequency'])\n",
    "            \n",
    "            fig = px.bar(\n",
    "                top_words, \n",
    "                x='Word', \n",
    "                y='Frequency',\n",
    "                title='Top 15 Topics in r/malaysia',\n",
    "                color_discrete_sequence=['darkblue']\n",
    "            )\n",
    "            \n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "    \n",
    "    # Footer\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"### Malaysian Reddit Sentiment Analysis Dashboard\")\n",
    "    st.markdown(\"*Data source: [r/malaysia](https://www.reddit.com/r/malaysia/)*\")\n",
    "    st.markdown(\"*Last updated: \" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"*\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee6c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Running the Streamlit App\n",
    "# You can run this cell to start the Streamlit app using the command below\n",
    "# !streamlit run malaysia_reddit_sentiment_dashboard.py\n",
    "\n",
    "# Display instructions for running the Streamlit app\n",
    "print(\"✅ Streamlit dashboard created: malaysia_reddit_sentiment_dashboard.py\")\n",
    "print(\"\\nTo run the dashboard:\")\n",
    "print(\"1. Make sure you've exported your data to 'malaysia_reddit_sentiment.csv'\")\n",
    "print(\"2. Open a terminal and navigate to your project directory\")\n",
    "print(\"3. Run the command: streamlit run malaysia_reddit_sentiment_dashboard.py\")\n",
    "print(\"\\nThe dashboard should open in your default web browser.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c55817",
   "metadata": {},
   "source": [
    "## 6. Conclusion and Next Steps\n",
    "\n",
    "In this project, we've built a comprehensive sentiment analysis pipeline for monitoring public sentiment from the r/malaysia subreddit. The pipeline includes:\n",
    "\n",
    "1. **Data Collection**: We set up Reddit API integration to stream posts from r/malaysia\n",
    "2. **Data Processing**: We implemented a Kafka producer-consumer architecture for real-time data processing\n",
    "3. **Sentiment Analysis**: We compared multiple sentiment analysis models (VADER, TextBlob, and Hugging Face Transformers)\n",
    "4. **Data Storage**: We stored the analyzed data in Elasticsearch for efficient querying\n",
    "5. **Visualization**: We created an interactive Streamlit dashboard for exploring sentiment insights\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "From our initial analysis, we can observe:\n",
    "- The distribution of positive, negative, and neutral sentiment in r/malaysia posts\n",
    "- How different sentiment analysis models compare in their evaluations\n",
    "- Trends in Malaysian public sentiment over time\n",
    "- Common topics and themes discussed in the Malaysian Reddit community\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "For future development of this project, consider:\n",
    "1. **Multilingual Support**: Add support for Malay language sentiment analysis\n",
    "2. **Custom Model Training**: Train a model specifically for Malaysian context and Manglish\n",
    "3. **Topic Modeling**: Implement more sophisticated topic extraction using techniques like LDA\n",
    "4. **Real-time Dashboard**: Connect the Streamlit dashboard directly to the Kafka stream\n",
    "5. **Expanded Data Sources**: Include other Malaysian social media sources like Twitter/X, Facebook, etc.\n",
    "\n",
    "### Final Deliverables\n",
    "\n",
    "To complete your project submission (due June 27, 2025), ensure you prepare:\n",
    "1. **Complete Source Code**: This notebook and additional Python scripts\n",
    "2. **Technical Report**: Implementation details, model performance, insights (PDF)\n",
    "3. **Presentation Slides**: For your 10-minute group presentation\n",
    "4. **GitHub Repository**: Organized codebase with documentation\n",
    "5. **Dashboard Screenshots**: Include visualizations from your Streamlit app\n",
    "\n",
    "This project demonstrates practical skills in big data processing, natural language processing, and interactive data visualization – all key competencies in the data science field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777350a2",
   "metadata": {},
   "source": [
    "## Appendix: Production Environment Setup\n",
    "\n",
    "For running this pipeline in a production environment with larger volumes of Reddit data, you would need the following setup:\n",
    "\n",
    "### Infrastructure Requirements\n",
    "\n",
    "1. **Apache Kafka Cluster**:\n",
    "   - 3+ nodes with at least 16GB RAM each\n",
    "   - SSD storage for better performance\n",
    "   - Kafka version 2.8.0 or newer\n",
    "   - Properly configured topics with appropriate partitioning\n",
    "\n",
    "2. **Apache Spark Cluster**:\n",
    "   - 1 master node (8+ cores, 32GB+ RAM)\n",
    "   - 3+ worker nodes (8+ cores, 32GB+ RAM each)\n",
    "   - Spark 3.3.0 or newer\n",
    "   - YARN or Kubernetes for resource management\n",
    "\n",
    "3. **Elasticsearch Cluster**:\n",
    "   - 3+ data nodes (8+ cores, 32GB+ RAM each)\n",
    "   - 1+ dedicated master node\n",
    "   - 1+ ingest node\n",
    "   - SSD storage for data nodes\n",
    "   - Elasticsearch 8.x\n",
    "\n",
    "4. **Monitoring and Logging**:\n",
    "   - Prometheus + Grafana for metrics monitoring\n",
    "   - ELK stack for log collection and analysis\n",
    "   - Healthchecks for critical services\n",
    "\n",
    "### Deployment Options\n",
    "\n",
    "1. **Cloud-based Deployment**:\n",
    "   - AWS: EMR for Spark, MSK for Kafka, Elasticsearch Service\n",
    "   - Azure: HDInsight, Event Hubs, Elasticsearch on Azure\n",
    "   - GCP: Dataproc, Pub/Sub, Elasticsearch on GCP\n",
    "\n",
    "2. **On-premises / Hybrid Deployment**:\n",
    "   - Docker containers with Kubernetes orchestration\n",
    "   - Ansible for configuration management\n",
    "   - Load balancers for service exposure\n",
    "\n",
    "### Scaling Considerations\n",
    "\n",
    "1. **Data Volume Scaling**:\n",
    "   - Increase Kafka partitions for parallel processing\n",
    "   - Shard Elasticsearch indices appropriately\n",
    "   - Configure proper retention policies\n",
    "\n",
    "2. **Processing Scaling**:\n",
    "   - Add Spark executors/workers as needed\n",
    "   - Tune memory allocation for executors\n",
    "   - Implement backpressure handling in Kafka consumers\n",
    "\n",
    "3. **API Rate Limiting**:\n",
    "   - Implement proper handling of Reddit API rate limits\n",
    "   - Add circuit breakers to prevent cascading failures\n",
    "   - Consider proxy rotation for higher volume data collection\n",
    "\n",
    "This setup would allow the sentiment analysis pipeline to handle hundreds of thousands of Reddit posts with near real-time analysis and visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
