{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484f0846",
   "metadata": {},
   "source": [
    "# Project 2 - Big Data & NLP Analytics Setup\n",
    "\n",
    "This notebook sets up a comprehensive environment for:\n",
    "- **Big Data Processing**: PySpark, Kafka\n",
    "- **Natural Language Processing**: NLTK, spaCy, Transformers, TextBlob\n",
    "- **Machine Learning**: PyTorch, Scikit-learn\n",
    "- **Data Analysis**: Pandas, NumPy\n",
    "- **Visualization**: Matplotlib, Seaborn, Plotly, WordCloud\n",
    "- **Web Scraping**: BeautifulSoup, Tweepy, Requests\n",
    "- **Search & Storage**: Elasticsearch\n",
    "- **Web Applications**: Streamlit\n",
    "\n",
    "## Installation Process\n",
    "Run the cells below to install all required libraries with specific versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9261551a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.5.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pyspark==3.5.0) (0.10.9.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python==2.0.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (2.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confluent-kafka==2.3.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (2.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark==2.0.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (2.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.8.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from nltk==3.8.1) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from nltk==3.8.1) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from nltk==3.8.1) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from nltk==3.8.1) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk==3.8.1) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy==3.7.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (2.32.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (78.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from spacy==3.7.2) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from spacy==3.7.2) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.2) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.2) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.2) (2025.6.15)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy==3.7.2) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy==3.7.2) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==3.7.2) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy==3.7.2) (8.2.1)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy==3.7.2) (0.16.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.2) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from jinja2->spacy==3.7.2) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.35.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from transformers==4.35.2) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from transformers==4.35.2) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from transformers==4.35.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from transformers==4.35.2) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from transformers==4.35.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from transformers==4.35.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from transformers==4.35.2) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from transformers==4.35.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from transformers==4.35.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from transformers==4.35.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers==4.35.2) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->transformers==4.35.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->transformers==4.35.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->transformers==4.35.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->transformers==4.35.2) (2025.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement torch==2.1.1 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1)\n",
      "ERROR: No matching distribution found for torch==2.1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.3.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from scikit-learn==1.3.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from scikit-learn==1.3.2) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from scikit-learn==1.3.2) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from scikit-learn==1.3.2) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob==0.17.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (0.17.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from textblob==0.17.1) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from nltk>=3.1->textblob==0.17.1) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from nltk>=3.1->textblob==0.17.1) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from nltk>=3.1->textblob==0.17.1) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from nltk>=3.1->textblob==0.17.1) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk>=3.1->textblob==0.17.1) (0.4.6)\n",
      "Requirement already satisfied: vaderSentiment==3.3.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from vaderSentiment==3.3.2) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->vaderSentiment==3.3.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->vaderSentiment==3.3.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->vaderSentiment==3.3.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->vaderSentiment==3.3.2) (2025.6.15)\n",
      "Requirement already satisfied: vaderSentiment==3.3.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from vaderSentiment==3.3.2) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->vaderSentiment==3.3.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->vaderSentiment==3.3.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->vaderSentiment==3.3.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests->vaderSentiment==3.3.2) (2025.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch==8.11.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (8.11.0)\n",
      "Requirement already satisfied: elastic-transport<9,>=8 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from elasticsearch==8.11.0) (8.17.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from elastic-transport<9,>=8->elasticsearch==8.11.0) (2.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from elastic-transport<9,>=8->elasticsearch==8.11.0) (2025.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch-dsl==8.11.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (8.11.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from elasticsearch-dsl==8.11.0) (2.9.0.post0)\n",
      "Requirement already satisfied: elasticsearch<9.0.0,>=8.0.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from elasticsearch-dsl==8.11.0) (8.11.0)\n",
      "Requirement already satisfied: elastic-transport<9,>=8 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from elasticsearch<9.0.0,>=8.0.0->elasticsearch-dsl==8.11.0) (8.17.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from elastic-transport<9,>=8->elasticsearch<9.0.0,>=8.0.0->elasticsearch-dsl==8.11.0) (2.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from elastic-transport<9,>=8->elasticsearch<9.0.0,>=8.0.0->elasticsearch-dsl==8.11.0) (2025.6.15)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil->elasticsearch-dsl==8.11.0) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.1.3 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (2.1.3)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pandas==2.1.3) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from pandas==2.1.3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pandas==2.1.3) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pandas==2.1.3) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas==2.1.3) (1.16.0)\n",
      "Collecting numpy==1.25.2\n",
      "  Using cached numpy-1.25.2.tar.gz (10.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n",
      "Collecting numpy==1.25.2\n",
      "  Using cached numpy-1.25.2.tar.gz (10.8 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [33 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 389, in <module>\n",
      "          main()\n",
      "        File \"C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 373, in main\n",
      "          json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 137, in get_requires_for_build_wheel\n",
      "          backend = _build_backend()\n",
      "                    ^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 70, in _build_backend\n",
      "          obj = import_module(mod_path)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "          return _bootstrap._gcd_import(name[level:], package, level)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "        File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "        File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "        File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "        File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "        File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "        File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "        File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "        File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
      "        File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "        File \"C:\\Users\\Honor\\AppData\\Local\\Temp\\pip-build-env-8ji88158\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 16, in <module>\n",
      "          import setuptools.version\n",
      "        File \"C:\\Users\\Honor\\AppData\\Local\\Temp\\pip-build-env-8ji88158\\overlay\\Lib\\site-packages\\setuptools\\version.py\", line 1, in <module>\n",
      "          import pkg_resources\n",
      "        File \"C:\\Users\\Honor\\AppData\\Local\\Temp\\pip-build-env-8ji88158\\overlay\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2172, in <module>\n",
      "          register_finder(pkgutil.ImpImporter, find_on_path)\n",
      "                          ^^^^^^^^^^^^^^^^^^^\n",
      "      AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib==3.8.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib==3.8.2) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib==3.8.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib==3.8.2) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib==3.8.2) (1.4.8)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib==3.8.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib==3.8.2) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib==3.8.2) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib==3.8.2) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib==3.8.2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib==3.8.2) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn==0.13.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from seaborn==0.13.0) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from seaborn==0.13.0) (2.1.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from seaborn==0.13.0) (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn==0.13.0) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn==0.13.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn==0.13.0) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn==0.13.0) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn==0.13.0) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn==0.13.0) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn==0.13.0) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib!=3.6.1,>=3.3->seaborn==0.13.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pandas>=1.2->seaborn==0.13.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pandas>=1.2->seaborn==0.13.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn==0.13.0) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly==5.17.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (5.17.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from plotly==5.17.0) (9.1.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from plotly==5.17.0) (24.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests==2.31.0\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests==2.31.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests==2.31.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests==2.31.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests==2.31.0) (2025.6.15)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Installing collected packages: requests\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.4\n",
      "    Uninstalling requests-2.32.4:\n",
      "      Successfully uninstalled requests-2.32.4\n",
      "Successfully installed requests-2.31.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "openai 0.28.0 requires aiohttp, which is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4==4.12.2\n",
      "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4==4.12.2)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "\n",
      "   ---------------------------------------- 0/2 [soupsieve]\n",
      "   ---------------------------------------- 0/2 [soupsieve]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   -------------------- ------------------- 1/2 [beautifulsoup4]\n",
      "   ---------------------------------------- 2/2 [beautifulsoup4]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.12.2 soupsieve-2.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy==4.14.0\n",
      "  Downloading tweepy-4.14.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting oauthlib<4,>=3.2.0 (from tweepy==4.14.0)\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from tweepy==4.14.0) (2.31.0)\n",
      "Collecting requests-oauthlib<2,>=1.2.0 (from tweepy==4.14.0)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3,>=2.27.0->tweepy==4.14.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3,>=2.27.0->tweepy==4.14.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3,>=2.27.0->tweepy==4.14.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3,>=2.27.0->tweepy==4.14.0) (2025.6.15)\n",
      "Downloading tweepy-4.14.0-py3-none-any.whl (98 kB)\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ---------------------------------------- 0/3 [oauthlib]\n",
      "   ------------- -------------------------- 1/3 [requests-oauthlib]\n",
      "   ------------- -------------------------- 1/3 [requests-oauthlib]\n",
      "   ------------- -------------------------- 1/3 [requests-oauthlib]\n",
      "   -------------------------- ------------- 2/3 [tweepy]\n",
      "   -------------------------- ------------- 2/3 [tweepy]\n",
      "   -------------------------- ------------- 2/3 [tweepy]\n",
      "   -------------------------- ------------- 2/3 [tweepy]\n",
      "   -------------------------- ------------- 2/3 [tweepy]\n",
      "   -------------------------- ------------- 2/3 [tweepy]\n",
      "   ---------------------------------------- 3/3 [tweepy]\n",
      "\n",
      "Successfully installed oauthlib-3.3.1 requests-oauthlib-1.3.1 tweepy-4.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv==1.0.0\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud==1.9.2\n",
      "  Downloading wordcloud-1.9.2.tar.gz (222 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [16 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 35, in <module>\n",
      "        File \"C:\\Users\\Honor\\AppData\\Local\\Temp\\pip-install-8hs0zz6k\\wordcloud_0c1e7efd406048149227bfd28aeaff34\\setup.py\", line 13, in <module>\n",
      "          version=versioneer.get_version(),\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Honor\\AppData\\Local\\Temp\\pip-install-8hs0zz6k\\wordcloud_0c1e7efd406048149227bfd28aeaff34\\versioneer.py\", line 1480, in get_version\n",
      "          return get_versions()[\"version\"]\n",
      "                 ^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Honor\\AppData\\Local\\Temp\\pip-install-8hs0zz6k\\wordcloud_0c1e7efd406048149227bfd28aeaff34\\versioneer.py\", line 1412, in get_versions\n",
      "          cfg = get_config_from_root(root)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Honor\\AppData\\Local\\Temp\\pip-install-8hs0zz6k\\wordcloud_0c1e7efd406048149227bfd28aeaff34\\versioneer.py\", line 342, in get_config_from_root\n",
      "          parser = configparser.SafeConfigParser()\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "      AttributeError: module 'configparser' has no attribute 'SafeConfigParser'. Did you mean: 'RawConfigParser'?\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit==1.28.2\n",
      "  Downloading streamlit-1.28.2-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit==1.28.2)\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit==1.28.2)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit==1.28.2)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from streamlit==1.28.2) (8.2.1)\n",
      "Collecting importlib-metadata<7,>=1.4 (from streamlit==1.28.2)\n",
      "  Downloading importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from streamlit==1.28.2) (1.26.4)\n",
      "Collecting packaging<24,>=16.8 (from streamlit==1.28.2)\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from streamlit==1.28.2) (2.1.3)\n",
      "Collecting pillow<11,>=7.1.0 (from streamlit==1.28.2)\n",
      "  Downloading pillow-10.4.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting protobuf<5,>=3.20 (from streamlit==1.28.2)\n",
      "  Downloading protobuf-4.25.8-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting pyarrow>=6.0 (from streamlit==1.28.2)\n",
      "  Downloading pyarrow-20.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from streamlit==1.28.2) (2.9.0.post0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from streamlit==1.28.2) (2.31.0)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit==1.28.2)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tenacity<9,>=8.1.0 (from streamlit==1.28.2)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit==1.28.2)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from streamlit==1.28.2) (4.14.0)\n",
      "Collecting tzlocal<6,>=1.1 (from streamlit==1.28.2)\n",
      "  Downloading tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting validators<1,>=0.2 (from streamlit==1.28.2)\n",
      "  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit==1.28.2)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit==1.28.2)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from streamlit==1.28.2) (6.4)\n",
      "Collecting watchdog>=2.1.5 (from streamlit==1.28.2)\n",
      "  Downloading watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from altair<6,>=4.0->streamlit==1.28.2) (3.1.6)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit==1.28.2)\n",
      "  Downloading jsonschema-4.24.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from altair<6,>=4.0->streamlit==1.28.2) (1.43.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from click<9,>=7.0->streamlit==1.28.2) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.28.2)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.28.2)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<7,>=1.4->streamlit==1.28.2)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit==1.28.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit==1.28.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil<3,>=2.7.3->streamlit==1.28.2) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3,>=2.27->streamlit==1.28.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3,>=2.27->streamlit==1.28.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3,>=2.27->streamlit==1.28.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from requests<3,>=2.27->streamlit==1.28.2) (2025.6.15)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from rich<14,>=10.14.0->streamlit==1.28.2) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from rich<14,>=10.14.0->streamlit==1.28.2) (2.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit==1.28.2) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\honor\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.28.2) (25.3.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.28.2)\n",
      "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.28.2)\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.28.2)\n",
      "  Downloading rpds_py-0.25.1-cp312-cp312-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\honor\\.conda\\envs\\env1\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit==1.28.2) (0.1.2)\n",
      "Downloading streamlit-1.28.2-py2.py3-none-any.whl (8.4 MB)\n",
      "   ---------------------------------------- 0.0/8.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/8.4 MB 10.1 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.1/8.4 MB 4.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.4/8.4 MB 3.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.1/8.4 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.2/8.4 MB 4.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 4.7/8.4 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.2/8.4 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.3/8.4 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.3/8.4 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.3/8.4 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.3/8.4 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.3/8.4 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.3/8.4 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.6/8.4 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.4/8.4 MB 2.5 MB/s eta 0:00:00\n",
      "Downloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "   ---------------------------------------- 0.0/731.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 731.2/731.2 kB 6.0 MB/s eta 0:00:00\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Downloading pillow-10.4.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 2.4/2.6 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 8.2 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.8-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "   ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 1.6/6.9 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.1/6.9 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.9/6.9 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.4/6.9 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.2/6.9 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.2/6.9 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.3/6.9 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.9/6.9 MB 4.1 MB/s eta 0:00:00\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Downloading validators-0.35.0-py3-none-any.whl (44 kB)\n",
      "Downloading jsonschema-4.24.0-py3-none-any.whl (88 kB)\n",
      "Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Downloading pyarrow-20.0.0-cp312-cp312-win_amd64.whl (25.7 MB)\n",
      "   ---------------------------------------- 0.0/25.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/25.7 MB 6.3 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.6/25.7 MB 6.3 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.6/25.7 MB 6.3 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.6/25.7 MB 6.3 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.6/25.7 MB 6.3 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 3.4/25.7 MB 2.6 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 4.2/25.7 MB 2.8 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 5.2/25.7 MB 3.0 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 5.2/25.7 MB 3.0 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 5.2/25.7 MB 3.0 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 5.2/25.7 MB 3.0 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 5.5/25.7 MB 2.1 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 6.3/25.7 MB 2.2 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 7.1/25.7 MB 2.3 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 8.4/25.7 MB 2.5 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 9.4/25.7 MB 2.7 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 11.0/25.7 MB 2.9 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 12.3/25.7 MB 3.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 13.6/25.7 MB 3.3 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 14.9/25.7 MB 3.4 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 15.7/25.7 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 16.5/25.7 MB 3.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 17.0/25.7 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 17.6/25.7 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 18.1/25.7 MB 3.3 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 18.9/25.7 MB 3.3 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.9/25.7 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 21.0/25.7 MB 3.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 22.3/25.7 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.3/25.7 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.4/25.7 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.2/25.7 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.7/25.7 MB 3.6 MB/s eta 0:00:00\n",
      "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.25.1-cp312-cp312-win_amd64.whl (235 kB)\n",
      "Downloading watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: zipp, watchdog, validators, tzlocal, toml, tenacity, smmap, rpds-py, pyarrow, protobuf, pillow, packaging, cachetools, blinker, rich, referencing, pydeck, importlib-metadata, gitdb, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
      "\n",
      "   - --------------------------------------  1/24 [watchdog]\n",
      "   - --------------------------------------  1/24 [watchdog]\n",
      "   - --------------------------------------  1/24 [watchdog]\n",
      "   - --------------------------------------  1/24 [watchdog]\n",
      "   - --------------------------------------  1/24 [watchdog]\n",
      "   --- ------------------------------------  2/24 [validators]\n",
      "   --- ------------------------------------  2/24 [validators]\n",
      "   --- ------------------------------------  2/24 [validators]\n",
      "   --- ------------------------------------  2/24 [validators]\n",
      "   --- ------------------------------------  2/24 [validators]\n",
      "   --- ------------------------------------  2/24 [validators]\n",
      "   ----- ----------------------------------  3/24 [tzlocal]\n",
      "  Attempting uninstall: tenacity\n",
      "   ----- ----------------------------------  3/24 [tzlocal]\n",
      "    Found existing installation: tenacity 9.1.2\n",
      "   ----- ----------------------------------  3/24 [tzlocal]\n",
      "    Uninstalling tenacity-9.1.2:\n",
      "   ----- ----------------------------------  3/24 [tzlocal]\n",
      "      Successfully uninstalled tenacity-9.1.2\n",
      "   ----- ----------------------------------  3/24 [tzlocal]\n",
      "   -------- -------------------------------  5/24 [tenacity]\n",
      "   -------- -------------------------------  5/24 [tenacity]\n",
      "   ---------- -----------------------------  6/24 [smmap]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   ------------- --------------------------  8/24 [pyarrow]\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "  Attempting uninstall: pillow\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "    Found existing installation: pillow 11.2.1\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "    Uninstalling pillow-11.2.1:\n",
      "   --------------- ------------------------  9/24 [protobuf]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "      Successfully uninstalled pillow-11.2.1\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "  Attempting uninstall: packaging\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "    Found existing installation: packaging 24.0\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "    Uninstalling packaging-24.0:\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "      Successfully uninstalled packaging-24.0\n",
      "   ---------------- ----------------------- 10/24 [pillow]\n",
      "   ------------------ --------------------- 11/24 [packaging]\n",
      "   ------------------ --------------------- 11/24 [packaging]\n",
      "   -------------------- ------------------- 12/24 [cachetools]\n",
      "  Attempting uninstall: rich\n",
      "   -------------------- ------------------- 12/24 [cachetools]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "    Found existing installation: rich 14.0.0\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "    Uninstalling rich-14.0.0:\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "      Successfully uninstalled rich-14.0.0\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ----------------------- ---------------- 14/24 [rich]\n",
      "   ------------------------- -------------- 15/24 [referencing]\n",
      "   ------------------------- -------------- 15/24 [referencing]\n",
      "   -------------------------- ------------- 16/24 [pydeck]\n",
      "   -------------------------- ------------- 16/24 [pydeck]\n",
      "   -------------------------- ------------- 16/24 [pydeck]\n",
      "   -------------------------- ------------- 16/24 [pydeck]\n",
      "   -------------------------- ------------- 16/24 [pydeck]\n",
      "   ---------------------------- ----------- 17/24 [importlib-metadata]\n",
      "   ------------------------------ --------- 18/24 [gitdb]\n",
      "   ------------------------------ --------- 18/24 [gitdb]\n",
      "   ------------------------------ --------- 18/24 [gitdb]\n",
      "   ------------------------------ --------- 18/24 [gitdb]\n",
      "   ------------------------------- -------- 19/24 [jsonschema-specifications]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   --------------------------------- ------ 20/24 [gitpython]\n",
      "   ----------------------------------- ---- 21/24 [jsonschema]\n",
      "   ----------------------------------- ---- 21/24 [jsonschema]\n",
      "   ----------------------------------- ---- 21/24 [jsonschema]\n",
      "   ----------------------------------- ---- 21/24 [jsonschema]\n",
      "   ----------------------------------- ---- 21/24 [jsonschema]\n",
      "   ----------------------------------- ---- 21/24 [jsonschema]\n",
      "   ----------------------------------- ---- 21/24 [jsonschema]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   ------------------------------------ --- 22/24 [altair]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   -------------------------------------- - 23/24 [streamlit]\n",
      "   ---------------------------------------- 24/24 [streamlit]\n",
      "\n",
      "Successfully installed altair-5.5.0 blinker-1.9.0 cachetools-5.5.2 gitdb-4.0.12 gitpython-3.1.44 importlib-metadata-6.11.0 jsonschema-4.24.0 jsonschema-specifications-2025.4.1 packaging-23.2 pillow-10.4.0 protobuf-4.25.8 pyarrow-20.0.0 pydeck-0.9.1 referencing-0.36.2 rich-13.9.4 rpds-py-0.25.1 smmap-5.0.2 streamlit-1.28.2 tenacity-8.5.0 toml-0.10.2 tzlocal-5.3.1 validators-0.35.0 watchdog-6.0.0 zipp-3.23.0\n",
      "✅ Installation completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~lotly (C:\\Users\\Honor\\.conda\\envs\\env1\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "modin 0.32.0 requires pandas<2.3,>=2.2, but you have pandas 2.1.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Simple Installation - Run this cell to install all libraries\n",
    "!pip install pyspark==3.5.0\n",
    "!pip install kafka-python==2.0.2\n",
    "!pip install confluent-kafka==2.3.0\n",
    "!pip install findspark==2.0.1\n",
    "!pip install nltk==3.8.1\n",
    "!pip install spacy==3.7.2\n",
    "!pip install transformers==4.35.2\n",
    "!pip install torch==2.1.1\n",
    "!pip install scikit-learn==1.3.2\n",
    "!pip install textblob==0.17.1\n",
    "!pip install vaderSentiment==3.3.2\n",
    "!pip install elasticsearch==8.11.0\n",
    "!pip install elasticsearch-dsl==8.11.0\n",
    "!pip install pandas==2.1.3\n",
    "!pip install numpy==1.25.2\n",
    "!pip install matplotlib==3.8.2\n",
    "!pip install seaborn==0.13.0\n",
    "\n",
    "!pip install plotly==5.17.0\n",
    "!pip install requests==2.31.0\n",
    "!pip install beautifulsoup4==4.12.2\n",
    "!pip install tweepy==4.14.0\n",
    "!pip install python-dotenv==1.0.0\n",
    "!pip install wordcloud==1.9.2\n",
    "!pip install streamlit==1.28.2\n",
    "\n",
    "print(\"✅ Installation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb624fc8",
   "metadata": {},
   "source": [
    "# Malaysian Reddit Sentiment Analysis Pipeline\n",
    "\n",
    "## Project Overview\n",
    "This project (worth 15% of course assessment) builds a real-time sentiment analysis pipeline monitoring public sentiment from r/malaysia subreddit. The system processes large-scale text data using Apache technologies, classifies sentiment, and visualizes insights through interactive dashboards.\n",
    "\n",
    "## 📋 Project Specifications\n",
    "- **Deadline**: Friday, June 27, 2025\n",
    "- **Team Size**: Maximum 4 students per group\n",
    "- **Data Source**: [r/malaysia](https://www.reddit.com/r/malaysia/)\n",
    "\n",
    "## 🎯 Core Objectives\n",
    "1. Monitor public sentiment in real-time from r/malaysia\n",
    "2. Process Reddit data using Apache Spark and Kafka\n",
    "3. Classify sentiment as positive, negative, or neutral\n",
    "4. Visualize insights through interactive dashboards\n",
    "\n",
    "## 📚 Components in this Notebook\n",
    "1. **Data Collection**: Streaming data from Reddit API\n",
    "2. **Data Processing Pipeline**: Kafka + Spark integration\n",
    "3. **Sentiment Analysis**: Multiple model implementations\n",
    "4. **Storage**: Elasticsearch integration \n",
    "5. **Visualization**: Real-time dashboards\n",
    "\n",
    "Let's start building our pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fa00c9",
   "metadata": {},
   "source": [
    "# Reddit Sentiment Analysis Workflow\n",
    "\n",
    "## 📋 Overview\n",
    "This workflow demonstrates real-time sentiment analysis using Reddit data from r/MalaysianFood, Apache Kafka for streaming, and includes both posts and comments collection.\n",
    "\n",
    "## 🏗️ Architecture\n",
    "```\n",
    "Reddit API → Reddit Producer → Kafka Topic → Consumer → Data Storage → Analysis\n",
    "```\n",
    "\n",
    "## 📦 What's Included\n",
    "- **reddit_raw_producer.py**: Enhanced Reddit data fetcher that collects both posts AND comments\n",
    "- **kafka_consumer.py**: Kafka message consumer with data persistence\n",
    "- **Sample Data**: Real exported data from r/MalaysianFood\n",
    "- **Setup Instructions**: Complete workflow setup guide\n",
    "\n",
    "## 🚀 Quick Start\n",
    "\n",
    "### 1. Install Dependencies\n",
    "```bash\n",
    "pip install praw kafka-python python-dotenv pandas\n",
    "```\n",
    "\n",
    "### 2. Setup Environment\n",
    "Copy `.env.sample` to `.env.local` and add your Reddit credentials:\n",
    "```env\n",
    "REDDIT_CLIENT_ID=your_reddit_client_id\n",
    "REDDIT_CLIENT_SECRET=your_reddit_client_secret\n",
    "REDDIT_USERNAME=your_reddit_username\n",
    "REDDIT_PASSWORD=your_reddit_password\n",
    "KAFKA_BOOTSTRAP_SERVERS=localhost:9092\n",
    "```\n",
    "\n",
    "### 3. Get Reddit API Credentials\n",
    "1. Go to https://www.reddit.com/prefs/apps\n",
    "2. Create new app (script type)\n",
    "3. Note client_id and client_secret\n",
    "\n",
    "### 4. Start Kafka & Zookeeper\n",
    "```bash\n",
    "# Start Zookeeper\n",
    "cd C:\\kafka\\kafka_2.13-3.9.1\n",
    "bin\\windows\\zookeeper-server-start.bat config\\zookeeper.properties\n",
    "\n",
    "# Start Kafka (new terminal)\n",
    "cd C:\\kafka\\kafka_2.13-3.9.1\n",
    "bin\\windows\\kafka-server-start.bat config\\server.properties\n",
    "```\n",
    "\n",
    "### 5. Run the Pipeline\n",
    "```bash\n",
    "# Terminal 1: Start Reddit Producer\n",
    "python reddit_pipeline/reddit_raw_producer.py\n",
    "\n",
    "# Terminal 2: Start Consumer\n",
    "python reddit_pipeline/reddit_raw_consumer.py\n",
    "```\n",
    "\n",
    "## 📊 What You'll Get\n",
    "\n",
    "### Enhanced Data Collection\n",
    "- **Posts**: Original submissions with metadata\n",
    "- **Comments**: User discussions and reactions (up to 10 per post)\n",
    "- **Comprehensive Coverage**: Multiple fetching strategies (new)\n",
    "\n",
    "### Data Format\n",
    "```json\n",
    "{\n",
    "  \"id\": \"1lg5wgx\",\n",
    "  \"title\": \"Braised Pork Belly Rice...\",\n",
    "  \"content\": \"Post or comment text\",\n",
    "  \"content_type\": \"post\", // or \"comment\"\n",
    "  \"parent_post_id\": \"1lg5wgx\", // for comments\n",
    "  \"score\": 9,\n",
    "  \"created_date\": \"2025-06-20T14:49:52+00:00\",\n",
    "  \"author\": \"username\",\n",
    "  \"subreddit\": \"MalaysianFood\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Expected Output\n",
    "- **500+ posts** from r/MalaysianFood\n",
    "- **1000+ comments** for rich sentiment data\n",
    "- **Real-time streaming** through Kafka\n",
    "- **Persistent storage** in JSONL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9613c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional dependencies for Reddit pipeline\n",
    "!pip install praw==7.7.1\n",
    "!pip install python-dotenv==1.0.0\n",
    "\n",
    "print(\"✅ Reddit-specific dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe1dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Create project directory structure\n",
    "project_dirs = [\n",
    "    'reddit_pipeline',\n",
    "    'data',\n",
    "    'data/raw',\n",
    "    'data/processed',\n",
    "    'config'\n",
    "]\n",
    "\n",
    "for dir_path in project_dirs:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"✅ Created directory: {dir_path}\")\n",
    "\n",
    "# Create environment sample file\n",
    "env_sample = \"\"\"# Reddit API Credentials\n",
    "REDDIT_CLIENT_ID=your_reddit_client_id\n",
    "REDDIT_CLIENT_SECRET=your_reddit_client_secret\n",
    "REDDIT_USERNAME=your_reddit_username\n",
    "REDDIT_PASSWORD=your_reddit_password\n",
    "\n",
    "# Kafka Configuration\n",
    "KAFKA_BOOTSTRAP_SERVERS=localhost:9092\n",
    "KAFKA_TOPIC=reddit_posts\n",
    "\n",
    "# Data Configuration\n",
    "SUBREDDIT=MalaysianFood\n",
    "MAX_POSTS=500\n",
    "MAX_COMMENTS_PER_POST=10\n",
    "\"\"\"\n",
    "\n",
    "with open('.env.sample', 'w') as f:\n",
    "    f.write(env_sample)\n",
    "\n",
    "print(\"✅ Created .env.sample file\")\n",
    "print(\"📝 Please copy .env.sample to .env.local and add your Reddit API credentials\")\n",
    "print(\"🔗 Get Reddit API credentials at: https://www.reddit.com/prefs/apps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb1710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reddit_raw_producer.py\n",
    "reddit_producer_code = '''\n",
    "import praw\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from kafka import KafkaProducer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('.env.local')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class RedditProducer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize Reddit and Kafka connections\"\"\"\n",
    "        self.setup_reddit()\n",
    "        self.setup_kafka()\n",
    "        \n",
    "    def setup_reddit(self):\n",
    "        \"\"\"Setup Reddit API connection\"\"\"\n",
    "        try:\n",
    "            self.reddit = praw.Reddit(\n",
    "                client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "                client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
    "                username=os.getenv('REDDIT_USERNAME'),\n",
    "                password=os.getenv('REDDIT_PASSWORD'),\n",
    "                user_agent='Malaysian Food Sentiment Analysis Bot v1.0'\n",
    "            )\n",
    "            logger.info(\"✅ Reddit API connected successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Reddit API connection failed: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def setup_kafka(self):\n",
    "        \"\"\"Setup Kafka producer\"\"\"\n",
    "        try:\n",
    "            self.producer = KafkaProducer(\n",
    "                bootstrap_servers=os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092'),\n",
    "                value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "                key_serializer=lambda k: k.encode('utf-8') if k else None\n",
    "            )\n",
    "            self.topic = os.getenv('KAFKA_TOPIC', 'reddit_posts')\n",
    "            logger.info(\"✅ Kafka producer connected successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Kafka connection failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def format_post_data(self, post):\n",
    "        \"\"\"Format Reddit post data\"\"\"\n",
    "        return {\n",
    "            'id': post.id,\n",
    "            'title': post.title,\n",
    "            'content': post.selftext if post.selftext else post.title,\n",
    "            'content_type': 'post',\n",
    "            'parent_post_id': post.id,\n",
    "            'score': post.score,\n",
    "            'upvote_ratio': post.upvote_ratio,\n",
    "            'num_comments': post.num_comments,\n",
    "            'created_date': datetime.fromtimestamp(post.created_utc, tz=timezone.utc).isoformat(),\n",
    "            'author': str(post.author) if post.author else '[deleted]',\n",
    "            'subreddit': str(post.subreddit),\n",
    "            'url': post.url,\n",
    "            'permalink': f\"https://reddit.com{post.permalink}\"\n",
    "        }\n",
    "    \n",
    "    def format_comment_data(self, comment, parent_post_id):\n",
    "        \"\"\"Format Reddit comment data\"\"\"\n",
    "        return {\n",
    "            'id': comment.id,\n",
    "            'title': f\"Comment on: {parent_post_id}\",\n",
    "            'content': comment.body,\n",
    "            'content_type': 'comment',\n",
    "            'parent_post_id': parent_post_id,\n",
    "            'score': comment.score,\n",
    "            'created_date': datetime.fromtimestamp(comment.created_utc, tz=timezone.utc).isoformat(),\n",
    "            'author': str(comment.author) if comment.author else '[deleted]',\n",
    "            'subreddit': str(comment.subreddit),\n",
    "            'permalink': f\"https://reddit.com{comment.permalink}\"\n",
    "        }\n",
    "    \n",
    "    def send_to_kafka(self, data):\n",
    "        \"\"\"Send data to Kafka topic\"\"\"\n",
    "        try:\n",
    "            self.producer.send(self.topic, key=data['id'], value=data)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to send to Kafka: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def collect_posts_and_comments(self, subreddit_name='MalaysianFood', max_posts=500, max_comments=10):\n",
    "        \"\"\"Collect posts and comments from subreddit\"\"\"\n",
    "        logger.info(f\"🚀 Starting data collection from r/{subreddit_name}\")\n",
    "        \n",
    "        try:\n",
    "            subreddit = self.reddit.subreddit(subreddit_name)\n",
    "            posts_collected = 0\n",
    "            comments_collected = 0\n",
    "            \n",
    "            # Collect from multiple sources for comprehensive coverage\n",
    "            post_sources = [\n",
    "                ('hot', subreddit.hot(limit=max_posts//3)),\n",
    "                ('new', subreddit.new(limit=max_posts//3)),\n",
    "                ('top', subreddit.top(time_filter='week', limit=max_posts//3))\n",
    "            ]\n",
    "            \n",
    "            processed_posts = set()\n",
    "            \n",
    "            for source_name, posts in post_sources:\n",
    "                logger.info(f\"📄 Collecting {source_name} posts...\")\n",
    "                \n",
    "                for post in posts:\n",
    "                    if post.id in processed_posts:\n",
    "                        continue\n",
    "                        \n",
    "                    processed_posts.add(post.id)\n",
    "                    \n",
    "                    # Process post\n",
    "                    post_data = self.format_post_data(post)\n",
    "                    if self.send_to_kafka(post_data):\n",
    "                        posts_collected += 1\n",
    "                        logger.info(f\"📤 Post sent: {post.title[:50]}...\")\n",
    "                    \n",
    "                    # Process comments\n",
    "                    try:\n",
    "                        post.comments.replace_more(limit=0)  # Remove \"more comments\"\n",
    "                        comments = post.comments.list()[:max_comments]\n",
    "                        \n",
    "                        for comment in comments:\n",
    "                            if hasattr(comment, 'body') and comment.body != '[deleted]':\n",
    "                                comment_data = self.format_comment_data(comment, post.id)\n",
    "                                if self.send_to_kafka(comment_data):\n",
    "                                    comments_collected += 1\n",
    "                                    \n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error processing comments for post {post.id}: {e}\")\n",
    "                    \n",
    "                    # Rate limiting\n",
    "                    time.sleep(0.1)\n",
    "                    \n",
    "                    if posts_collected >= max_posts:\n",
    "                        break\n",
    "                \n",
    "                if posts_collected >= max_posts:\n",
    "                    break\n",
    "            \n",
    "            logger.info(f\"✅ Collection completed!\")\n",
    "            logger.info(f\"📊 Posts collected: {posts_collected}\")\n",
    "            logger.info(f\"💬 Comments collected: {comments_collected}\")\n",
    "            logger.info(f\"📝 Total items: {posts_collected + comments_collected}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Collection failed: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.producer.close()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        producer = RedditProducer()\n",
    "        producer.collect_posts_and_comments(\n",
    "            subreddit_name=os.getenv('SUBREDDIT', 'MalaysianFood'),\n",
    "            max_posts=int(os.getenv('MAX_POSTS', 500)),\n",
    "            max_comments=int(os.getenv('MAX_COMMENTS_PER_POST', 10))\n",
    "        )\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"🛑 Collection stopped by user\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Application failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Write the producer script\n",
    "with open('reddit_pipeline/reddit_raw_producer.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(reddit_producer_code)\n",
    "\n",
    "print(\"✅ Created reddit_raw_producer.py\")\n",
    "print(\"📦 Features included:\")\n",
    "print(\"  - Collects both posts AND comments\")\n",
    "print(\"  - Multiple fetching strategies (hot, new, top)\")\n",
    "print(\"  - Comprehensive data format\")\n",
    "print(\"  - Rate limiting and error handling\")\n",
    "print(\"  - Real-time Kafka streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3b6238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reddit_raw_consumer.py\n",
    "reddit_consumer_code = '''\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "from kafka import KafkaConsumer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('.env.local')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class RedditConsumer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize Kafka consumer\"\"\"\n",
    "        self.setup_kafka()\n",
    "        self.setup_data_files()\n",
    "        \n",
    "    def setup_kafka(self):\n",
    "        \"\"\"Setup Kafka consumer\"\"\"\n",
    "        try:\n",
    "            self.consumer = KafkaConsumer(\n",
    "                os.getenv('KAFKA_TOPIC', 'reddit_posts'),\n",
    "                bootstrap_servers=os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092'),\n",
    "                value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "                key_deserializer=lambda k: k.decode('utf-8') if k else None,\n",
    "                auto_offset_reset='earliest',\n",
    "                enable_auto_commit=True,\n",
    "                group_id='reddit_consumer_group'\n",
    "            )\n",
    "            logger.info(\"✅ Kafka consumer connected successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Kafka consumer connection failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def setup_data_files(self):\n",
    "        \"\"\"Setup data storage files\"\"\"\n",
    "        os.makedirs('data/raw', exist_ok=True)\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        self.posts_file = f'data/raw/reddit_posts_{timestamp}.jsonl'\n",
    "        self.comments_file = f'data/raw/reddit_comments_{timestamp}.jsonl'\n",
    "        logger.info(f\"📁 Data will be saved to:\")\n",
    "        logger.info(f\"  Posts: {self.posts_file}\")\n",
    "        logger.info(f\"  Comments: {self.comments_file}\")\n",
    "    \n",
    "    def save_data(self, data):\n",
    "        \"\"\"Save data to appropriate file based on content type\"\"\"\n",
    "        try:\n",
    "            if data['content_type'] == 'post':\n",
    "                with open(self.posts_file, 'a', encoding='utf-8') as f:\n",
    "                    f.write(json.dumps(data, ensure_ascii=False) + '\\\\n')\n",
    "            elif data['content_type'] == 'comment':\n",
    "                with open(self.comments_file, 'a', encoding='utf-8') as f:\n",
    "                    f.write(json.dumps(data, ensure_ascii=False) + '\\\\n')\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save data: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def process_messages(self):\n",
    "        \"\"\"Process messages from Kafka\"\"\"\n",
    "        logger.info(\"🔄 Starting message processing...\")\n",
    "        logger.info(\"💡 Press Ctrl+C to stop the consumer\")\n",
    "        \n",
    "        posts_count = 0\n",
    "        comments_count = 0\n",
    "        \n",
    "        try:\n",
    "            for message in self.consumer:\n",
    "                data = message.value\n",
    "                \n",
    "                # Save data\n",
    "                if self.save_data(data):\n",
    "                    if data['content_type'] == 'post':\n",
    "                        posts_count += 1\n",
    "                        logger.info(f\"📄 Post saved: {data['title'][:50]}... (Total posts: {posts_count})\")\n",
    "                    elif data['content_type'] == 'comment':\n",
    "                        comments_count += 1\n",
    "                        logger.info(f\"💬 Comment saved: {data['content'][:50]}... (Total comments: {comments_count})\")\n",
    "                \n",
    "                # Log progress every 50 items\n",
    "                total_items = posts_count + comments_count\n",
    "                if total_items % 50 == 0:\n",
    "                    logger.info(f\"📊 Progress: {posts_count} posts, {comments_count} comments ({total_items} total)\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"🛑 Consumer stopped by user\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Consumer error: {e}\")\n",
    "        finally:\n",
    "            self.consumer.close()\n",
    "            logger.info(f\"✅ Final count: {posts_count} posts, {comments_count} comments\")\n",
    "            logger.info(f\"📁 Data saved to:\")\n",
    "            logger.info(f\"  Posts: {self.posts_file}\")\n",
    "            logger.info(f\"  Comments: {self.comments_file}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        consumer = RedditConsumer()\n",
    "        consumer.process_messages()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Application failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Write the consumer script\n",
    "with open('reddit_pipeline/reddit_raw_consumer.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(reddit_consumer_code)\n",
    "\n",
    "print(\"✅ Created reddit_raw_consumer.py\")\n",
    "print(\"📦 Features included:\")\n",
    "print(\"  - Separate storage for posts and comments\")\n",
    "print(\"  - JSONL format for easy processing\")\n",
    "print(\"  - Real-time progress logging\")\n",
    "print(\"  - Graceful shutdown handling\")\n",
    "print(\"  - Timestamped file names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e3752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for testing\n",
    "sample_data = [\n",
    "    {\n",
    "        \"id\": \"1lg5wgx\",\n",
    "        \"title\": \"Braised Pork Belly Rice from a Local Restaurant\",\n",
    "        \"content\": \"Just had this amazing braised pork belly rice. The meat was so tender and flavorful!\",\n",
    "        \"content_type\": \"post\",\n",
    "        \"parent_post_id\": \"1lg5wgx\",\n",
    "        \"score\": 15,\n",
    "        \"upvote_ratio\": 0.94,\n",
    "        \"num_comments\": 8,\n",
    "        \"created_date\": \"2025-06-20T14:49:52+00:00\",\n",
    "        \"author\": \"foodie_kl\",\n",
    "        \"subreddit\": \"MalaysianFood\",\n",
    "        \"url\": \"https://reddit.com/r/MalaysianFood/...\",\n",
    "        \"permalink\": \"/r/MalaysianFood/comments/1lg5wgx/braised_pork_belly_rice/\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"comment123\",\n",
    "        \"title\": \"Comment on: 1lg5wgx\",\n",
    "        \"content\": \"This looks absolutely delicious! Where is this restaurant located?\",\n",
    "        \"content_type\": \"comment\",\n",
    "        \"parent_post_id\": \"1lg5wgx\",\n",
    "        \"score\": 3,\n",
    "        \"created_date\": \"2025-06-20T15:22:11+00:00\",\n",
    "        \"author\": \"hungrytraveler\",\n",
    "        \"subreddit\": \"MalaysianFood\",\n",
    "        \"permalink\": \"/r/MalaysianFood/comments/1lg5wgx/braised_pork_belly_rice/comment123/\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"1lh8abc\",\n",
    "        \"title\": \"Best Nasi Lemak in Penang?\",\n",
    "        \"content\": \"Planning a food trip to Penang. Any recommendations for authentic nasi lemak?\",\n",
    "        \"content_type\": \"post\",\n",
    "        \"parent_post_id\": \"1lh8abc\",\n",
    "        \"score\": 22,\n",
    "        \"upvote_ratio\": 0.96,\n",
    "        \"num_comments\": 15,\n",
    "        \"created_date\": \"2025-06-21T09:15:30+00:00\",\n",
    "        \"author\": \"penang_explorer\",\n",
    "        \"subreddit\": \"MalaysianFood\",\n",
    "        \"url\": \"https://reddit.com/r/MalaysianFood/...\",\n",
    "        \"permalink\": \"/r/MalaysianFood/comments/1lh8abc/best_nasi_lemak_penang/\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save sample data\n",
    "os.makedirs('data/sample', exist_ok=True)\n",
    "with open('data/sample/malaysian_food_sample.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sample_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ Created sample data file\")\n",
    "print(f\"📊 Sample includes {len(sample_data)} items:\")\n",
    "for item in sample_data:\n",
    "    print(f\"  - {item['content_type'].title()}: {item['title'][:50]}...\")\n",
    "\n",
    "# Create README for the pipeline\n",
    "readme_content = '''# Reddit Sentiment Analysis Pipeline\n",
    "\n",
    "## 🚀 Quick Setup Guide\n",
    "\n",
    "### 1. Environment Setup\n",
    "```bash\n",
    "# Copy environment template\n",
    "copy .env.sample .env.local\n",
    "\n",
    "# Edit .env.local with your credentials\n",
    "notepad .env.local\n",
    "```\n",
    "\n",
    "### 2. Start Infrastructure\n",
    "```bash\n",
    "# Terminal 1: Start Zookeeper\n",
    "cd C:\\\\kafka\\\\kafka_2.13-3.9.1\n",
    "bin\\\\windows\\\\zookeeper-server-start.bat config\\\\zookeeper.properties\n",
    "\n",
    "# Terminal 2: Start Kafka\n",
    "cd C:\\\\kafka\\\\kafka_2.13-3.9.1\n",
    "bin\\\\windows\\\\kafka-server-start.bat config\\\\server.properties\n",
    "```\n",
    "\n",
    "### 3. Run Pipeline\n",
    "```bash\n",
    "# Terminal 3: Start Producer\n",
    "python reddit_pipeline/reddit_raw_producer.py\n",
    "\n",
    "# Terminal 4: Start Consumer\n",
    "python reddit_pipeline/reddit_raw_consumer.py\n",
    "```\n",
    "\n",
    "## 📁 File Structure\n",
    "```\n",
    "project/\n",
    "├── reddit_pipeline/\n",
    "│   ├── reddit_raw_producer.py  # Reddit data collector\n",
    "│   └── reddit_raw_consumer.py  # Kafka consumer\n",
    "├── data/\n",
    "│   ├── raw/                    # Raw Reddit data\n",
    "│   ├── processed/              # Processed data\n",
    "│   └── sample/                 # Sample data\n",
    "├── .env.sample                 # Environment template\n",
    "└── .env.local                  # Your credentials (create this)\n",
    "```\n",
    "\n",
    "## 🔧 Configuration\n",
    "\n",
    "### Reddit API Setup\n",
    "1. Go to https://www.reddit.com/prefs/apps\n",
    "2. Create new app (script type)\n",
    "3. Note client_id and client_secret\n",
    "4. Add to .env.local\n",
    "\n",
    "### Environment Variables\n",
    "- `REDDIT_CLIENT_ID`: Your Reddit app client ID\n",
    "- `REDDIT_CLIENT_SECRET`: Your Reddit app secret\n",
    "- `REDDIT_USERNAME`: Your Reddit username\n",
    "- `REDDIT_PASSWORD`: Your Reddit password\n",
    "- `KAFKA_BOOTSTRAP_SERVERS`: Kafka server (default: localhost:9092)\n",
    "- `KAFKA_TOPIC`: Kafka topic name (default: reddit_posts)\n",
    "- `SUBREDDIT`: Target subreddit (default: MalaysianFood)\n",
    "- `MAX_POSTS`: Maximum posts to collect (default: 500)\n",
    "- `MAX_COMMENTS_PER_POST`: Max comments per post (default: 10)\n",
    "\n",
    "## 📊 Expected Output\n",
    "- 500+ posts from r/MalaysianFood\n",
    "- 1000+ comments for sentiment analysis\n",
    "- Real-time streaming through Kafka\n",
    "- Persistent storage in JSONL format\n",
    "'''\n",
    "\n",
    "with open('README.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"✅ Created README.md with setup instructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d096f2b",
   "metadata": {},
   "source": [
    "## 🔍 Sentiment Analysis Implementation\n",
    "\n",
    "Now let's implement the sentiment analysis component that will process the collected Reddit data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20382311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('vader_lexicon', quiet=True)\n",
    "    print(\"✅ NLTK data downloaded\")\n",
    "except:\n",
    "    print(\"⚠️ NLTK download failed, continuing...\")\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize sentiment analysis models\"\"\"\n",
    "        self.vader = SentimentIntensityAnalyzer()\n",
    "        print(\"✅ Sentiment analyzer initialized\")\n",
    "    \n",
    "    def analyze_textblob(self, text):\n",
    "        \"\"\"Analyze sentiment using TextBlob\"\"\"\n",
    "        try:\n",
    "            blob = TextBlob(text)\n",
    "            polarity = blob.sentiment.polarity\n",
    "            \n",
    "            if polarity > 0.1:\n",
    "                sentiment = 'positive'\n",
    "            elif polarity < -0.1:\n",
    "                sentiment = 'negative'\n",
    "            else:\n",
    "                sentiment = 'neutral'\n",
    "            \n",
    "            return {\n",
    "                'sentiment': sentiment,\n",
    "                'polarity': polarity,\n",
    "                'subjectivity': blob.sentiment.subjectivity\n",
    "            }\n",
    "        except:\n",
    "            return {'sentiment': 'neutral', 'polarity': 0.0, 'subjectivity': 0.0}\n",
    "    \n",
    "    def analyze_vader(self, text):\n",
    "        \"\"\"Analyze sentiment using VADER\"\"\"\n",
    "        try:\n",
    "            scores = self.vader.polarity_scores(text)\n",
    "            compound = scores['compound']\n",
    "            \n",
    "            if compound >= 0.05:\n",
    "                sentiment = 'positive'\n",
    "            elif compound <= -0.05:\n",
    "                sentiment = 'negative'\n",
    "            else:\n",
    "                sentiment = 'neutral'\n",
    "            \n",
    "            return {\n",
    "                'sentiment': sentiment,\n",
    "                'compound': compound,\n",
    "                'positive': scores['pos'],\n",
    "                'negative': scores['neg'],\n",
    "                'neutral': scores['neu']\n",
    "            }\n",
    "        except:\n",
    "            return {\n",
    "                'sentiment': 'neutral', \n",
    "                'compound': 0.0,\n",
    "                'positive': 0.0,\n",
    "                'negative': 0.0,\n",
    "                'neutral': 1.0\n",
    "            }\n",
    "    \n",
    "    def analyze_comprehensive(self, text):\n",
    "        \"\"\"Comprehensive sentiment analysis using multiple models\"\"\"\n",
    "        textblob_result = self.analyze_textblob(text)\n",
    "        vader_result = self.analyze_vader(text)\n",
    "        \n",
    "        # Combine results for ensemble prediction\n",
    "        sentiments = [textblob_result['sentiment'], vader_result['sentiment']]\n",
    "        \n",
    "        # Majority vote or weighted average\n",
    "        if sentiments.count('positive') > sentiments.count('negative'):\n",
    "            final_sentiment = 'positive'\n",
    "        elif sentiments.count('negative') > sentiments.count('positive'):\n",
    "            final_sentiment = 'negative'\n",
    "        else:\n",
    "            final_sentiment = 'neutral'\n",
    "        \n",
    "        return {\n",
    "            'final_sentiment': final_sentiment,\n",
    "            'textblob': textblob_result,\n",
    "            'vader': vader_result,\n",
    "            'confidence': abs(textblob_result['polarity']) + abs(vader_result['compound']) / 2\n",
    "        }\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = SentimentAnalyzer()\n",
    "\n",
    "# Test with sample data\n",
    "test_texts = [\n",
    "    \"This nasi lemak is absolutely delicious! Best I've ever had!\",\n",
    "    \"The food was terrible and overpriced. Would not recommend.\",\n",
    "    \"The restaurant is okay, nothing special but decent service.\",\n",
    "    \"Amazing curry laksa! Will definitely come back for more.\"\n",
    "]\n",
    "\n",
    "print(\"🧪 Testing sentiment analysis:\")\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    result = analyzer.analyze_comprehensive(text)\n",
    "    print(f\"{i}. Text: {text[:50]}...\")\n",
    "    print(f\"   Sentiment: {result['final_sentiment']} (confidence: {result['confidence']:.2f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a3087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reddit_data(file_path):\n",
    "    \"\"\"Process Reddit data from JSONL file\"\"\"\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    data.append(json.loads(line))\n",
    "        print(f\"✅ Loaded {len(data)} items from {file_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ File not found: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def analyze_sentiment_batch(df, analyzer):\n",
    "    \"\"\"Analyze sentiment for a batch of Reddit data\"\"\"\n",
    "    print(\"🔄 Processing sentiment analysis...\")\n",
    "    \n",
    "    sentiments = []\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['content']\n",
    "        if pd.isna(text) or text == '':\n",
    "            text = row['title']\n",
    "        \n",
    "        result = analyzer.analyze_comprehensive(str(text))\n",
    "        sentiments.append(result)\n",
    "        \n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(df)} items...\")\n",
    "    \n",
    "    # Add sentiment results to dataframe\n",
    "    df['sentiment'] = [s['final_sentiment'] for s in sentiments]\n",
    "    df['sentiment_confidence'] = [s['confidence'] for s in sentiments]\n",
    "    df['textblob_polarity'] = [s['textblob']['polarity'] for s in sentiments]\n",
    "    df['vader_compound'] = [s['vader']['compound'] for s in sentiments]\n",
    "    \n",
    "    print(f\"✅ Sentiment analysis completed for {len(df)} items\")\n",
    "    return df\n",
    "\n",
    "def create_sentiment_report(df):\n",
    "    \"\"\"Create comprehensive sentiment analysis report\"\"\"\n",
    "    if len(df) == 0:\n",
    "        print(\"❌ No data to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n📊 SENTIMENT ANALYSIS REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_items = len(df)\n",
    "    sentiment_counts = df['sentiment'].value_counts()\n",
    "    \n",
    "    print(f\"📈 Total Items Analyzed: {total_items}\")\n",
    "    print(f\"📋 Content Type Breakdown:\")\n",
    "    print(df['content_type'].value_counts().to_string())\n",
    "    print(f\"\\n🎭 Sentiment Distribution:\")\n",
    "    for sentiment, count in sentiment_counts.items():\n",
    "        percentage = (count / total_items) * 100\n",
    "        print(f\"  {sentiment.title()}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Average sentiment scores\n",
    "    print(f\"\\n📊 Average Sentiment Scores:\")\n",
    "    print(f\"  TextBlob Polarity: {df['textblob_polarity'].mean():.3f}\")\n",
    "    print(f\"  VADER Compound: {df['vader_compound'].mean():.3f}\")\n",
    "    print(f\"  Confidence: {df['sentiment_confidence'].mean():.3f}\")\n",
    "    \n",
    "    # Content type analysis\n",
    "    print(f\"\\n📝 Sentiment by Content Type:\")\n",
    "    content_sentiment = df.groupby(['content_type', 'sentiment']).size().unstack(fill_value=0)\n",
    "    print(content_sentiment.to_string())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test with sample data\n",
    "print(\"🧪 Testing with sample data...\")\n",
    "sample_df = pd.DataFrame(sample_data)\n",
    "sample_analyzed = analyze_sentiment_batch(sample_df, analyzer)\n",
    "create_sentiment_report(sample_analyzed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95856fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "def create_sentiment_visualizations(df):\n",
    "    \"\"\"Create comprehensive sentiment visualizations\"\"\"\n",
    "    if len(df) == 0:\n",
    "        print(\"❌ No data to visualize\")\n",
    "        return\n",
    "    \n",
    "    print(\"📊 Creating sentiment visualizations...\")\n",
    "    \n",
    "    # 1. Sentiment Distribution Pie Chart\n",
    "    sentiment_counts = df['sentiment'].value_counts()\n",
    "    colors = {'positive': '#2E8B57', 'negative': '#DC143C', 'neutral': '#4682B4'}\n",
    "    \n",
    "    fig1 = px.pie(\n",
    "        values=sentiment_counts.values,\n",
    "        names=sentiment_counts.index,\n",
    "        title='Overall Sentiment Distribution',\n",
    "        color=sentiment_counts.index,\n",
    "        color_discrete_map=colors\n",
    "    )\n",
    "    fig1.show()\n",
    "    \n",
    "    # 2. Sentiment by Content Type\n",
    "    fig2 = px.histogram(\n",
    "        df, \n",
    "        x='content_type', \n",
    "        color='sentiment',\n",
    "        title='Sentiment Distribution by Content Type',\n",
    "        color_discrete_map=colors,\n",
    "        barmode='group'\n",
    "    )\n",
    "    fig2.show()\n",
    "    \n",
    "    # 3. Sentiment Scores Distribution\n",
    "    fig3 = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('TextBlob Polarity', 'VADER Compound'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    fig3.add_trace(\n",
    "        go.Histogram(x=df['textblob_polarity'], name='TextBlob', nbinsx=20),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig3.add_trace(\n",
    "        go.Histogram(x=df['vader_compound'], name='VADER', nbinsx=20),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig3.update_layout(title_text=\"Sentiment Score Distributions\")\n",
    "    fig3.show()\n",
    "    \n",
    "    # 4. Sentiment Over Time (if date data available)\n",
    "    if 'created_date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['created_date']).dt.date\n",
    "        daily_sentiment = df.groupby(['date', 'sentiment']).size().unstack(fill_value=0)\n",
    "        \n",
    "        fig4 = px.line(\n",
    "            daily_sentiment.reset_index(),\n",
    "            x='date',\n",
    "            y=['positive', 'negative', 'neutral'],\n",
    "            title='Sentiment Trends Over Time',\n",
    "            color_discrete_map=colors\n",
    "        )\n",
    "        fig4.show()\n",
    "    \n",
    "    # 5. Top Words by Sentiment (simple word frequency)\n",
    "    from collections import Counter\n",
    "    import re\n",
    "    \n",
    "    def get_top_words(texts, top_n=10):\n",
    "        all_words = []\n",
    "        for text in texts:\n",
    "            words = re.findall(r'\\b\\w+\\b', str(text).lower())\n",
    "            words = [w for w in words if len(w) > 3]  # Filter short words\n",
    "            all_words.extend(words)\n",
    "        return Counter(all_words).most_common(top_n)\n",
    "    \n",
    "    fig5 = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=('Positive', 'Negative', 'Neutral'),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    "    )\n",
    "    \n",
    "    for i, sentiment in enumerate(['positive', 'negative', 'neutral'], 1):\n",
    "        sentiment_texts = df[df['sentiment'] == sentiment]['content']\n",
    "        top_words = get_top_words(sentiment_texts)\n",
    "        \n",
    "        if top_words:\n",
    "            words, counts = zip(*top_words)\n",
    "            fig5.add_trace(\n",
    "                go.Bar(x=list(words), y=list(counts), name=sentiment),\n",
    "                row=1, col=i\n",
    "            )\n",
    "    \n",
    "    fig5.update_layout(title_text=\"Top Words by Sentiment\", showlegend=False)\n",
    "    fig5.show()\n",
    "    \n",
    "    print(\"✅ Visualizations created successfully!\")\n",
    "\n",
    "# Test visualization with sample data\n",
    "create_sentiment_visualizations(sample_analyzed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb34e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create complete pipeline runner\n",
    "pipeline_runner_code = '''\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sentiment_analyzer import SentimentAnalyzer\n",
    "from reddit_raw_consumer import RedditConsumer\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class RealTimeSentimentPipeline:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize real-time sentiment analysis pipeline\"\"\"\n",
    "        self.analyzer = SentimentAnalyzer()\n",
    "        self.processed_data = []\n",
    "        self.setup_output_files()\n",
    "        \n",
    "    def setup_output_files(self):\n",
    "        \"\"\"Setup output files for processed data\"\"\"\n",
    "        os.makedirs('data/processed', exist_ok=True)\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        self.output_file = f'data/processed/sentiment_analysis_{timestamp}.jsonl'\n",
    "        logger.info(f\"📁 Results will be saved to: {self.output_file}\")\n",
    "    \n",
    "    def process_message(self, data):\n",
    "        \"\"\"Process a single message with sentiment analysis\"\"\"\n",
    "        try:\n",
    "            # Extract text content\n",
    "            text = data.get('content', '') or data.get('title', '')\n",
    "            \n",
    "            if not text or text.strip() == '':\n",
    "                return None\n",
    "            \n",
    "            # Perform sentiment analysis\n",
    "            sentiment_result = self.analyzer.analyze_comprehensive(text)\n",
    "            \n",
    "            # Combine original data with sentiment results\n",
    "            processed_item = {\n",
    "                **data,\n",
    "                'sentiment': sentiment_result['final_sentiment'],\n",
    "                'sentiment_confidence': sentiment_result['confidence'],\n",
    "                'textblob_polarity': sentiment_result['textblob']['polarity'],\n",
    "                'textblob_subjectivity': sentiment_result['textblob']['subjectivity'],\n",
    "                'vader_compound': sentiment_result['vader']['compound'],\n",
    "                'vader_positive': sentiment_result['vader']['positive'],\n",
    "                'vader_negative': sentiment_result['vader']['negative'],\n",
    "                'vader_neutral': sentiment_result['vader']['neutral'],\n",
    "                'processed_timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # Save to file\n",
    "            with open(self.output_file, 'a', encoding='utf-8') as f:\n",
    "                f.write(json.dumps(processed_item, ensure_ascii=False) + '\\\\n')\n",
    "            \n",
    "            self.processed_data.append(processed_item)\n",
    "            \n",
    "            # Log progress\n",
    "            if len(self.processed_data) % 25 == 0:\n",
    "                sentiment_counts = {}\n",
    "                for item in self.processed_data:\n",
    "                    sentiment = item['sentiment']\n",
    "                    sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1\n",
    "                \n",
    "                logger.info(f\"📊 Processed {len(self.processed_data)} items - {sentiment_counts}\")\n",
    "            \n",
    "            return processed_item\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing message: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_real_time_stats(self):\n",
    "        \"\"\"Get real-time statistics\"\"\"\n",
    "        if not self.processed_data:\n",
    "            return {}\n",
    "        \n",
    "        df = pd.DataFrame(self.processed_data)\n",
    "        \n",
    "        stats = {\n",
    "            'total_processed': len(df),\n",
    "            'sentiment_distribution': df['sentiment'].value_counts().to_dict(),\n",
    "            'avg_confidence': df['sentiment_confidence'].mean(),\n",
    "            'content_type_breakdown': df['content_type'].value_counts().to_dict(),\n",
    "            'last_updated': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main pipeline execution\"\"\"\n",
    "    logger.info(\"🚀 Starting Real-Time Sentiment Analysis Pipeline\")\n",
    "    \n",
    "    pipeline = RealTimeSentimentPipeline()\n",
    "    \n",
    "    # Process existing data files if available\n",
    "    data_dir = 'data/raw'\n",
    "    if os.path.exists(data_dir):\n",
    "        for filename in os.listdir(data_dir):\n",
    "            if filename.endswith('.jsonl'):\n",
    "                filepath = os.path.join(data_dir, filename)\n",
    "                logger.info(f\"📄 Processing existing file: {filename}\")\n",
    "                \n",
    "                try:\n",
    "                    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                        for line in f:\n",
    "                            if line.strip():\n",
    "                                data = json.loads(line)\n",
    "                                pipeline.process_message(data)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing file {filename}: {e}\")\n",
    "    \n",
    "    # Print final statistics\n",
    "    stats = pipeline.get_real_time_stats()\n",
    "    if stats:\n",
    "        logger.info(\"📊 FINAL PIPELINE STATISTICS\")\n",
    "        logger.info(f\"  Total Processed: {stats['total_processed']}\")\n",
    "        logger.info(f\"  Sentiment Distribution: {stats['sentiment_distribution']}\")\n",
    "        logger.info(f\"  Average Confidence: {stats['avg_confidence']:.3f}\")\n",
    "        logger.info(f\"  Output File: {pipeline.output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Create sentiment analyzer module\n",
    "sentiment_analyzer_code = '''\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize sentiment analysis models\"\"\"\n",
    "        self.vader = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def analyze_textblob(self, text):\n",
    "        \"\"\"Analyze sentiment using TextBlob\"\"\"\n",
    "        try:\n",
    "            blob = TextBlob(text)\n",
    "            polarity = blob.sentiment.polarity\n",
    "            \n",
    "            if polarity > 0.1:\n",
    "                sentiment = 'positive'\n",
    "            elif polarity < -0.1:\n",
    "                sentiment = 'negative'\n",
    "            else:\n",
    "                sentiment = 'neutral'\n",
    "            \n",
    "            return {\n",
    "                'sentiment': sentiment,\n",
    "                'polarity': polarity,\n",
    "                'subjectivity': blob.sentiment.subjectivity\n",
    "            }\n",
    "        except:\n",
    "            return {'sentiment': 'neutral', 'polarity': 0.0, 'subjectivity': 0.0}\n",
    "    \n",
    "    def analyze_vader(self, text):\n",
    "        \"\"\"Analyze sentiment using VADER\"\"\"\n",
    "        try:\n",
    "            scores = self.vader.polarity_scores(text)\n",
    "            compound = scores['compound']\n",
    "            \n",
    "            if compound >= 0.05:\n",
    "                sentiment = 'positive'\n",
    "            elif compound <= -0.05:\n",
    "                sentiment = 'negative'\n",
    "            else:\n",
    "                sentiment = 'neutral'\n",
    "            \n",
    "            return {\n",
    "                'sentiment': sentiment,\n",
    "                'compound': compound,\n",
    "                'positive': scores['pos'],\n",
    "                'negative': scores['neg'],\n",
    "                'neutral': scores['neu']\n",
    "            }\n",
    "        except:\n",
    "            return {\n",
    "                'sentiment': 'neutral', \n",
    "                'compound': 0.0,\n",
    "                'positive': 0.0,\n",
    "                'negative': 0.0,\n",
    "                'neutral': 1.0\n",
    "            }\n",
    "    \n",
    "    def analyze_comprehensive(self, text):\n",
    "        \"\"\"Comprehensive sentiment analysis using multiple models\"\"\"\n",
    "        textblob_result = self.analyze_textblob(text)\n",
    "        vader_result = self.analyze_vader(text)\n",
    "        \n",
    "        # Combine results for ensemble prediction\n",
    "        sentiments = [textblob_result['sentiment'], vader_result['sentiment']]\n",
    "        \n",
    "        # Majority vote\n",
    "        if sentiments.count('positive') > sentiments.count('negative'):\n",
    "            final_sentiment = 'positive'\n",
    "        elif sentiments.count('negative') > sentiments.count('positive'):\n",
    "            final_sentiment = 'negative'\n",
    "        else:\n",
    "            final_sentiment = 'neutral'\n",
    "        \n",
    "        return {\n",
    "            'final_sentiment': final_sentiment,\n",
    "            'textblob': textblob_result,\n",
    "            'vader': vader_result,\n",
    "            'confidence': (abs(textblob_result['polarity']) + abs(vader_result['compound'])) / 2\n",
    "        }\n",
    "'''\n",
    "\n",
    "# Write the files\n",
    "with open('reddit_pipeline/sentiment_analyzer.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(sentiment_analyzer_code)\n",
    "\n",
    "with open('reddit_pipeline/realtime_pipeline.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(pipeline_runner_code)\n",
    "\n",
    "print(\"✅ Created real-time sentiment analysis pipeline\")\n",
    "print(\"📦 Components created:\")\n",
    "print(\"  - sentiment_analyzer.py: Core sentiment analysis module\")\n",
    "print(\"  - realtime_pipeline.py: Real-time processing pipeline\")\n",
    "print(\"  - Integration with Kafka consumer\")\n",
    "print(\"  - Automatic statistics generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee05fd0",
   "metadata": {},
   "source": [
    "## 🚀 Pipeline Execution Guide\n",
    "\n",
    "### Step-by-Step Execution\n",
    "\n",
    "1. **Setup Environment** (Run once)\n",
    "   - Install dependencies\n",
    "   - Create directory structure\n",
    "   - Configure environment variables\n",
    "\n",
    "2. **Start Infrastructure** (4 terminals)\n",
    "   ```bash\n",
    "   # Terminal 1: Zookeeper\n",
    "   cd C:\\kafka\\kafka_2.13-3.9.1\n",
    "   bin\\windows\\zookeeper-server-start.bat config\\zookeeper.properties\n",
    "   \n",
    "   # Terminal 2: Kafka\n",
    "   cd C:\\kafka\\kafka_2.13-3.9.1\n",
    "   bin\\windows\\kafka-server-start.bat config\\server.properties\n",
    "   \n",
    "   # Terminal 3: Reddit Producer\n",
    "   python reddit_pipeline/reddit_raw_producer.py\n",
    "   \n",
    "   # Terminal 4: Consumer + Sentiment Analysis\n",
    "   python reddit_pipeline/realtime_pipeline.py\n",
    "   ```\n",
    "\n",
    "3. **Monitor Results**\n",
    "   - Check `data/raw/` for raw Reddit data\n",
    "   - Check `data/processed/` for sentiment analysis results\n",
    "   - View real-time logs for progress\n",
    "\n",
    "### Expected Output Files\n",
    "\n",
    "```\n",
    "data/\n",
    "├── raw/\n",
    "│   ├── reddit_posts_YYYYMMDD_HHMMSS.jsonl\n",
    "│   └── reddit_comments_YYYYMMDD_HHMMSS.jsonl\n",
    "├── processed/\n",
    "│   └── sentiment_analysis_YYYYMMDD_HHMMSS.jsonl\n",
    "└── sample/\n",
    "    └── malaysian_food_sample.json\n",
    "```\n",
    "\n",
    "### 📊 What You'll Achieve\n",
    "\n",
    "- **500+ Reddit posts** from r/MalaysianFood\n",
    "- **1000+ comments** for comprehensive sentiment data\n",
    "- **Real-time sentiment classification** (positive/negative/neutral)\n",
    "- **Multiple sentiment models** (TextBlob + VADER)\n",
    "- **Interactive visualizations** using Plotly\n",
    "- **Persistent data storage** in JSONL format\n",
    "- **Comprehensive reporting** with statistics\n",
    "\n",
    "This pipeline demonstrates end-to-end big data processing with Apache Kafka, real-time sentiment analysis, and data visualization - perfect for your Big Data & NLP Analytics project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba320faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of the complete pipeline with sample data\n",
    "print(\"🧪 TESTING COMPLETE PIPELINE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Test directory structure\n",
    "print(\"📁 Directory structure:\")\n",
    "for root, dirs, files in os.walk('.'):\n",
    "    level = root.replace('.', '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        if not file.startswith('.'):\n",
    "            print(f\"{subindent}{file}\")\n",
    "\n",
    "print(\"\\n✅ Pipeline components created successfully!\")\n",
    "\n",
    "# 2. Test sample data processing\n",
    "print(\"\\n📊 Processing sample data through complete pipeline:\")\n",
    "sample_df = pd.DataFrame(sample_data)\n",
    "processed_df = analyze_sentiment_batch(sample_df, analyzer)\n",
    "\n",
    "# 3. Generate final report\n",
    "create_sentiment_report(processed_df)\n",
    "\n",
    "# 4. Save processed sample data\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "output_file = 'data/processed/sample_sentiment_results.json'\n",
    "processed_df.to_json(output_file, orient='records', indent=2)\n",
    "print(f\"\\n💾 Sample results saved to: {output_file}\")\n",
    "\n",
    "print(\"\\n🎉 PIPELINE READY FOR EXECUTION!\")\n",
    "print(\"📋 Next steps:\")\n",
    "print(\"  1. Copy .env.sample to .env.local\")\n",
    "print(\"  2. Add your Reddit API credentials\") \n",
    "print(\"  3. Start Kafka infrastructure\")\n",
    "print(\"  4. Run the pipeline scripts\")\n",
    "print(\"  5. Monitor results in data/ folders\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
